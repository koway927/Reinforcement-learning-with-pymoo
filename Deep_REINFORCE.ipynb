{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Policy(nn.Module):\n",
    "  def __init__(self,input_shape):\n",
    "    super().__init__()\n",
    "    print(\"input_shape\",input_shape)\n",
    "    self.model = nn.Sequential(\n",
    "        nn.Linear(input_shape[0],32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32,32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32,32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32,32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32,32),\n",
    "    )\n",
    "    self.mean = nn.Sequential(nn.Linear(32, input_shape[0]),\n",
    "                                  nn.Tanh())                    # tanh squashed output to the range of -1..1\n",
    "    self.variance =nn.Sequential(nn.Linear(32, input_shape[0]),\n",
    "                                     nn.Softplus())\n",
    "  def forward(self,x):\n",
    "    x = self.model(x)\n",
    "    return self.mean(x), self.variance(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.core.algorithm import Algorithm\n",
    "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
    "from torch.distributions import Normal, Uniform\n",
    "from pymoo.core.initialization import Initialization\n",
    "from pymoo.algorithms.moo.nsga2 import RankAndCrowdingSurvival\n",
    "from pymoo.operators.crossover.sbx import SimulatedBinaryCrossover\n",
    "from pymoo.core.population import Population\n",
    "from pymoo.operators.repair.bounds_repair import is_out_of_bounds_by_problem\n",
    "from pymoo.core.repair import NoRepair\n",
    "from torch import optim\n",
    "import torch\n",
    "import numpy as np\n",
    "from pymoo.util.optimum import filter_optimum\n",
    "\n",
    "class MonteCarloGradientPolicyAlgorithm(Algorithm):\n",
    "    def __init__(self,\n",
    "                 gamma=0.99,\n",
    "                 alpha=0.01,\n",
    "                 num_rounds=200,\n",
    "                 sample_size=500,\n",
    "                 sampling=FloatRandomSampling(),\n",
    "                 repair=NoRepair(),\n",
    "                 **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : \n",
    "            The environment to be used in the algorithm.\n",
    "        policy : {Policy}\n",
    "            The policy to be used in the algorithm.\n",
    "        gamma : float, optional\n",
    "            The discount factor used in the algorithm. The default is 0.99.\n",
    "        alpha : float, optional\n",
    "            The learning rate used in the algorithm. The default is 0.01.\n",
    "        num_episodes : int, optional\n",
    "            The number of episodes to be run in the algorithm. The default is 100.\n",
    "        sample_size : int, optional\n",
    "            The number of samples to be generated from the problems and used in the acquisition function. \n",
    "            The default is 10.\n",
    "        sampling : {Sampling}, optional\n",
    "            The sampling method used to generate the initial samples. The default is FloatRandomSampling().\n",
    "        \"\"\"\n",
    "         \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.num_rounds = num_rounds\n",
    "        self.sample_size = sample_size\n",
    "        self.sampling = sampling\n",
    "        self.repair = repair\n",
    "        \n",
    "        self.initialization = Initialization(sampling)\n",
    "        self.survival = RankAndCrowdingSurvival()\n",
    "        self.crossover = SimulatedBinaryCrossover(n_offsprings=1)\n",
    "        self.is_constraint_model = False\n",
    "        self.optimizer = None\n",
    "        self.data_set_X = None\n",
    "        self.model = None\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.rewards = []\n",
    "        self.steps_taken = []\n",
    "        \n",
    "\n",
    "    def _setup(self, problem, **kwargs):\n",
    "        if self.problem.n_ieq_constr + self.problem.n_eq_constr > 0:\n",
    "            self.is_constraint_model = True\n",
    "        self.model = Policy(np.array([self.problem.n_var]))\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 5e-3)\n",
    "        self.mean = self.problem.xl + (self.problem.xu - self.problem.xl)/2\n",
    "        self.std = np.sqrt(((self.problem.xu - self.problem.xl)**2)/12)\n",
    "        \n",
    "    def _initialize_infill(self):\n",
    "        return self.initialization.do(self.problem, 1, algorithm=self)\n",
    "\n",
    "    def _initialize_advance(self, infills=None, **kwargs):\n",
    "        self.data_set_X = self.pop.get(\"X\")\n",
    "        #super()._initialize_advance(infills=infills, **kwargs)\n",
    "\n",
    "    def _infill(self):\n",
    "        print(self.pop.size)\n",
    "        \n",
    "        state = self.get_starting_point()\n",
    "        normalized_state = self.state_normalization(state)\n",
    "        print(\"normalized_state1\",normalized_state)\n",
    "        termination_rounds = self.num_rounds \n",
    "        steps = 0\n",
    "        ep_rewards = 0\n",
    "        batch_rewards = []\n",
    "        log_probs = []\n",
    "\n",
    "        while steps < termination_rounds:\n",
    "            #print(\"self.opt.get(F)\",self.opt.get(\"F\"))\n",
    "           # print(\"self.problem.evaluate(np.array(state))[0]\",self.problem.evaluate(np.array(state))[0])\n",
    "            a, log_p = self.action(torch.Tensor(normalized_state).unsqueeze(0))\n",
    "            log_probs.append(log_p)\n",
    "            new_state, reward = self.step(a,normalized_state)\n",
    "            batch_rewards.append(reward)\n",
    "            ep_rewards += reward\n",
    "            steps +=1\n",
    "            #print(\"state:\", state)\n",
    "            state = new_state\n",
    "            #if steps + 1 == termination_rounds  and np.any(batch_rewards[-100:]) and termination_rounds  < 15000:\n",
    "                #print(\"steps:\", steps)\n",
    "                #print(\"batch_rewards[-100:]\", batch_rewards[-100:])\n",
    "                #termination_rounds  += 100\n",
    "        print(\"normalized_state2\",normalized_state)\n",
    "        self.data_set_X = np.vstack((self.data_set_X, normalized_state))\n",
    "        print(\"self.data_set_X\",self.data_set_X)\n",
    "        off = Population.new(X=self.data_set_X, F = self.problem.evaluate(self.data_set_X, return_values_of=[\"F\"]))\n",
    "        self.pop = off\n",
    "        self.rewards.append(ep_rewards)\n",
    "        self.steps_taken.append(steps)\n",
    "        print(\"Episode: {} --- Rewards: {} --- Steps: {}\".format(self.n_iter, ep_rewards, steps))\n",
    "        self.update_policy(self.n_iter, self.optimizer, batch_rewards, log_probs)\n",
    "\n",
    "        \n",
    "        return self.pop\n",
    "        \n",
    "    def _advance(self, infills=None, **kwargs):\n",
    "        return super()._advance(infills=infills, **kwargs)\n",
    "    \n",
    "    \n",
    "    def _finalize(self):\n",
    "        return super()._finalize()\n",
    "    \n",
    "    def action(self, state):\n",
    "        # simple pytorch aproach for action-selection and log-prob calc \n",
    "        #action_parameters = model(s)\n",
    "        mu,variance = self.model(state)\n",
    "        sigma = torch.sqrt(variance)\n",
    "        #print(\"action_parameters:\", action_parameters)\n",
    "        #m = Categorical(prob)\n",
    "        #print(\"action_parameters[:, :1]:\", action_parameters[:, :1])\n",
    "        #print(\"action_parameters[:, 1:]:\", action_parameters[:, 1:])\n",
    "        #mu, sigma = action_parameters[:, :1], torch.exp(action_parameters[:, 1:])\n",
    "        #mu, c = action_parameters[:, :1], 1\n",
    "        \n",
    "        #print(\"mu:\", mu)\n",
    "        #print(\"sigma:\", sigma)\n",
    "        m = Normal(mu, sigma+0.00001)\n",
    "        #print(\"m:\", m)\n",
    "        a = m.sample()\n",
    "        #print(\"a:\", a.tolist())\n",
    "        # log p(a∣π(s))\n",
    "        log_p = m.log_prob(a)\n",
    "        #print(\"log_p:\", log_p)\n",
    "        #print(a.item(), log_p)\n",
    "        return a.tolist(), log_p\n",
    "    \n",
    "    def get_rewards(self, current_state, new_state):\n",
    "        eucli_dist = self.problem.evaluate(current_state, return_values_of=[\"F\"]) - self.problem.evaluate(new_state, return_values_of=[\"F\"])\n",
    "        #print(\"self.problem.evaluate(new_state)[0]\",self.problem.evaluate(new_state)[0])\n",
    "        #print(\"self.problem.evaluate(current_state)[0]\",self.problem.evaluate(current_state)[0])\n",
    "        Y_cv = 0\n",
    "        if self.is_constraint_model : \n",
    "            Y_cv = self.problem.evaluate(new_state, return_values_of=[\"CV\"])    \n",
    "        if Y_cv > 0 or len(is_out_of_bounds_by_problem(self.problem, [new_state])) > 0:\n",
    "            return -3*eucli_dist# + self.penalty_function(new_state) + Y_cv**2)\n",
    "        elif eucli_dist < 0:\n",
    "            return -3*eucli_dist\n",
    "        elif eucli_dist > 0:\n",
    "            return 3*eucli_dist\n",
    "        elif eucli_dist == 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1*eucli_dist\n",
    "    \n",
    "    def penalty_function(self, state):\n",
    "        return  np.sum(np.maximum(0, state - self.problem.xu)**2) + np.sum(np.maximum(0, self.problem.xl - state)**2)\n",
    "    \n",
    "    def get_starting_point(self):\n",
    "        if self.pop.size <= 2 or np.random.random_sample() > 0.5:\n",
    "            return self.initialization.do(self.problem, 1, algorithm=self).get(\"X\")[0]\n",
    "        else:\n",
    "            new_parents = self.survival.do(self.problem, self.pop, n_survive=2)\n",
    "            new_state = self.crossover.do(self.problem, [new_parents]).get(\"X\")[0]\n",
    "            return new_state\n",
    "    def state_normalization(self, state):\n",
    "        return (state - self.mean)/self.std\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action, state):\n",
    "        current_X = state\n",
    "        X_new = current_X[0] + action[0]\n",
    "        reward = 0\n",
    "        #print(\"current_X:\", current_X)\n",
    "        #print(\"action:\", action)\n",
    "        while len(is_out_of_bounds_by_problem(self.problem, [X_new])) > 0:           \n",
    "            X_new = self.sampling.do(self.problem, 1, algorithm=self).get(\"X\")[0]\n",
    "        reward = self.get_rewards(current_X, X_new)\n",
    "        \n",
    "        \n",
    "        #if self.is_constraint_model or self.problem.n_eq_constr > 0:\n",
    "        #    Y_new = evlaution_of_new_points[0]\n",
    "        #    Constraint_new = evlaution_of_new_points[1]\n",
    "        #else:\n",
    "        \n",
    "        new_state = np.array(X_new)\n",
    "        #print(\"rewards\",reward)\n",
    "        return new_state, reward\n",
    "    \n",
    "    def update_policy(self, ep, optimizer,batch_rewards,log_probs):\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        rewards = []\n",
    "        #calc discounted Rewards\n",
    "        for r in batch_rewards[::-1]: # reverses the list of rewards \n",
    "            R = r + self.gamma * R\n",
    "            rewards.insert(0, R) # inserts the current reward to first position\n",
    "        \n",
    "        \n",
    "        rewards = torch.tensor(rewards)\n",
    "        # standardization to get data of zero mean and varianz 1, stabilizes learning \n",
    "        #-- attention scaling rewards looses information of special events with higher rewards - addapting on different environments  \n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + ep)\n",
    "        for log_prob, reward in zip(log_probs, rewards):\n",
    "            policy_loss.append(self.alpha*(-log_prob * reward)) #baseline+\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        print(\" policy_loss\", policy_loss)\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(),1)\n",
    "        optimizer.step()\n",
    "        \"\"\"G = 0\n",
    "        for t in reversed(range(len(states))):\n",
    "            G = self.gamma * G + rewards[t]\n",
    "            state = states[t]\n",
    "            action = actions[t]\n",
    "            grad_log_prob = self.policy.grad_log_prob(state, action)\n",
    "            self.policy.theta += self.alpha * G * grad_log_prob\"\"\"\n",
    "\n",
    "    \"\"\"def run_episode(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.policy.sample_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        return states, actions, rewards\"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape [5]\n",
      "1\n",
      "normalized_state1 [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      "normalized_state2 [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]]\n",
      "Episode: 2 --- Rewards: [7860647.28918521] --- Steps: 200\n",
      " policy_loss tensor(0.4855, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "2\n",
      "normalized_state1 [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      "normalized_state2 [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]]\n",
      "Episode: 3 --- Rewards: [7914753.10274915] --- Steps: 200\n",
      " policy_loss tensor(-0.2388, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "3\n",
      "normalized_state1 [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      "normalized_state2 [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]]\n",
      "Episode: 4 --- Rewards: [7713673.06241541] --- Steps: 200\n",
      " policy_loss tensor(-0.3532, dtype=torch.float64, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rayko/.venv/lib/python3.8/site-packages/pymoo/operators/crossover/sbx.py:47: RuntimeWarning: invalid value encountered in power\n",
      "  betaq[mask] = np.power((rand * alpha), (1.0 / (eta + 1.0)))[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "normalized_state1 [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      "normalized_state2 [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]]\n",
      "Episode: 5 --- Rewards: [7915968.63641472] --- Steps: 200\n",
      " policy_loss tensor(0.5068, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "5\n",
      "normalized_state1 [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      "normalized_state2 [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]]\n",
      "Episode: 6 --- Rewards: [7786217.17665714] --- Steps: 200\n",
      " policy_loss tensor(0.0273, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "6\n",
      "normalized_state1 [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      "normalized_state2 [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]]\n",
      "Episode: 7 --- Rewards: [8018489.50205123] --- Steps: 200\n",
      " policy_loss tensor(-0.2928, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "7\n",
      "normalized_state1 [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      "normalized_state2 [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      " [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]]\n",
      "Episode: 8 --- Rewards: [7858216.13082064] --- Steps: 200\n",
      " policy_loss tensor(-0.1836, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "8\n",
      "normalized_state1 [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]\n",
      "normalized_state2 [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      " [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]]\n",
      "Episode: 9 --- Rewards: [7963911.73204789] --- Steps: 200\n",
      " policy_loss tensor(0.1037, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "9\n",
      "normalized_state1 [-1.73205081  0.76322643 -1.7316546  -1.73205081 -1.22367349]\n",
      "normalized_state2 [-1.73205081  0.76322643 -1.7316546  -1.73205081 -1.22367349]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      " [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]\n",
      " [-1.73205081  0.76322643 -1.7316546  -1.73205081 -1.22367349]]\n",
      "Episode: 10 --- Rewards: [7783991.6929742] --- Steps: 200\n",
      " policy_loss tensor(0.3021, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "10\n",
      "normalized_state1 [-0.28744421  1.13054614 -1.13242098 -0.62942718 -1.22367349]\n",
      "normalized_state2 [-0.28744421  1.13054614 -1.13242098 -0.62942718 -1.22367349]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      " [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]\n",
      " [-1.73205081  0.76322643 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.28744421  1.13054614 -1.13242098 -0.62942718 -1.22367349]]\n",
      "Episode: 11 --- Rewards: [7972719.28119869] --- Steps: 200\n",
      " policy_loss tensor(-0.0465, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "11\n",
      "normalized_state1 [ 0.64477805 -0.11420814 -1.73205081 -1.73205081 -1.73205081]\n",
      "normalized_state2 [ 0.64477805 -0.11420814 -1.73205081 -1.73205081 -1.73205081]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      " [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]\n",
      " [-1.73205081  0.76322643 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.28744421  1.13054614 -1.13242098 -0.62942718 -1.22367349]\n",
      " [ 0.64477805 -0.11420814 -1.73205081 -1.73205081 -1.73205081]]\n",
      "Episode: 12 --- Rewards: [7892597.08238198] --- Steps: 200\n",
      " policy_loss tensor(0.0488, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "12\n",
      "normalized_state1 [-1.7166081   0.96483469  0.30191852 -0.1402908   0.16622052]\n",
      "normalized_state2 [-1.7166081   0.96483469  0.30191852 -0.1402908   0.16622052]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      " [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]\n",
      " [-1.73205081  0.76322643 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.28744421  1.13054614 -1.13242098 -0.62942718 -1.22367349]\n",
      " [ 0.64477805 -0.11420814 -1.73205081 -1.73205081 -1.73205081]\n",
      " [-1.7166081   0.96483469  0.30191852 -0.1402908   0.16622052]]\n",
      "Episode: 13 --- Rewards: [7912092.18216648] --- Steps: 200\n",
      " policy_loss tensor(0.4079, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "13\n",
      "normalized_state1 [ 1.34964832  0.09136813 -1.33021354 -0.47843384 -0.62279503]\n",
      "normalized_state2 [ 1.34964832  0.09136813 -1.33021354 -0.47843384 -0.62279503]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      " [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]\n",
      " [-1.73205081  0.76322643 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.28744421  1.13054614 -1.13242098 -0.62942718 -1.22367349]\n",
      " [ 0.64477805 -0.11420814 -1.73205081 -1.73205081 -1.73205081]\n",
      " [-1.7166081   0.96483469  0.30191852 -0.1402908   0.16622052]\n",
      " [ 1.34964832  0.09136813 -1.33021354 -0.47843384 -0.62279503]]\n",
      "Episode: 14 --- Rewards: [7754914.19948617] --- Steps: 200\n",
      " policy_loss tensor(0.1086, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "14\n",
      "normalized_state1 [-0.78337293  0.41884383 -1.36568547  1.20031187  1.13863902]\n",
      "normalized_state2 [-0.78337293  0.41884383 -1.36568547  1.20031187  1.13863902]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      " [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]\n",
      " [-1.73205081  0.76322643 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.28744421  1.13054614 -1.13242098 -0.62942718 -1.22367349]\n",
      " [ 0.64477805 -0.11420814 -1.73205081 -1.73205081 -1.73205081]\n",
      " [-1.7166081   0.96483469  0.30191852 -0.1402908   0.16622052]\n",
      " [ 1.34964832  0.09136813 -1.33021354 -0.47843384 -0.62279503]\n",
      " [-0.78337293  0.41884383 -1.36568547  1.20031187  1.13863902]]\n",
      "Episode: 15 --- Rewards: [7938121.60223882] --- Steps: 200\n",
      " policy_loss tensor(0.0774, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "15\n",
      "normalized_state1 [-0.28744421  0.76322643 -1.73205081 -0.68474005 -1.22367349]\n",
      "normalized_state2 [-0.28744421  0.76322643 -1.73205081 -0.68474005 -1.22367349]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      " [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]\n",
      " [-1.73205081  0.76322643 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.28744421  1.13054614 -1.13242098 -0.62942718 -1.22367349]\n",
      " [ 0.64477805 -0.11420814 -1.73205081 -1.73205081 -1.73205081]\n",
      " [-1.7166081   0.96483469  0.30191852 -0.1402908   0.16622052]\n",
      " [ 1.34964832  0.09136813 -1.33021354 -0.47843384 -0.62279503]\n",
      " [-0.78337293  0.41884383 -1.36568547  1.20031187  1.13863902]\n",
      " [-0.28744421  0.76322643 -1.73205081 -0.68474005 -1.22367349]]\n",
      "Episode: 16 --- Rewards: [7866944.21140624] --- Steps: 200\n",
      " policy_loss tensor(0.1701, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "16\n",
      "normalized_state1 [-1.11679529 -1.14939382 -0.47757318 -1.39602593  1.70063881]\n",
      "normalized_state2 [-1.11679529 -1.14939382 -0.47757318 -1.39602593  1.70063881]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      " [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]\n",
      " [-1.73205081  0.76322643 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.28744421  1.13054614 -1.13242098 -0.62942718 -1.22367349]\n",
      " [ 0.64477805 -0.11420814 -1.73205081 -1.73205081 -1.73205081]\n",
      " [-1.7166081   0.96483469  0.30191852 -0.1402908   0.16622052]\n",
      " [ 1.34964832  0.09136813 -1.33021354 -0.47843384 -0.62279503]\n",
      " [-0.78337293  0.41884383 -1.36568547  1.20031187  1.13863902]\n",
      " [-0.28744421  0.76322643 -1.73205081 -0.68474005 -1.22367349]\n",
      " [-1.11679529 -1.14939382 -0.47757318 -1.39602593  1.70063881]]\n",
      "Episode: 17 --- Rewards: [7804739.36701928] --- Steps: 200\n",
      " policy_loss tensor(-0.3319, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "17\n",
      "normalized_state1 [-0.247178   -1.73205081 -1.73205081 -0.69121898 -1.31669878]\n",
      "normalized_state2 [-0.247178   -1.73205081 -1.73205081 -0.69121898 -1.31669878]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      " [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]\n",
      " [-1.73205081  0.76322643 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.28744421  1.13054614 -1.13242098 -0.62942718 -1.22367349]\n",
      " [ 0.64477805 -0.11420814 -1.73205081 -1.73205081 -1.73205081]\n",
      " [-1.7166081   0.96483469  0.30191852 -0.1402908   0.16622052]\n",
      " [ 1.34964832  0.09136813 -1.33021354 -0.47843384 -0.62279503]\n",
      " [-0.78337293  0.41884383 -1.36568547  1.20031187  1.13863902]\n",
      " [-0.28744421  0.76322643 -1.73205081 -0.68474005 -1.22367349]\n",
      " [-1.11679529 -1.14939382 -0.47757318 -1.39602593  1.70063881]\n",
      " [-0.247178   -1.73205081 -1.73205081 -0.69121898 -1.31669878]]\n",
      "Episode: 18 --- Rewards: [7980235.52111171] --- Steps: 200\n",
      " policy_loss tensor(-0.6373, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "18\n",
      "normalized_state1 [-1.13088205 -1.08458758 -0.11063849  0.65406975  1.39594268]\n",
      "normalized_state2 [-1.13088205 -1.08458758 -0.11063849  0.65406975  1.39594268]\n",
      "self.data_set_X [[88.00852811 41.64389392 27.00205875 32.44198631 29.64160603]\n",
      " [-1.41218053 -1.08682651 -0.53499333 -0.35760796  0.13446511]\n",
      " [ 0.59010982 -0.24085281  0.92764823  0.12473708 -1.59397197]\n",
      " [-0.28744421 -1.73205081 -1.7316546  -0.68474005 -1.22367349]\n",
      " [-1.10770356  1.51918403  1.63072635 -0.67728298  1.25568972]\n",
      " [ 0.58387684  1.27483131 -1.05179811  0.73713529 -1.37212681]\n",
      " [-1.30384321 -0.85778343  1.21366299 -1.67584797 -1.19078154]\n",
      " [-1.73205081 -1.73205081 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.88519473  0.60054511 -1.4267593  -1.6667454  -0.09579553]\n",
      " [-1.73205081  0.76322643 -1.7316546  -1.73205081 -1.22367349]\n",
      " [-0.28744421  1.13054614 -1.13242098 -0.62942718 -1.22367349]\n",
      " [ 0.64477805 -0.11420814 -1.73205081 -1.73205081 -1.73205081]\n",
      " [-1.7166081   0.96483469  0.30191852 -0.1402908   0.16622052]\n",
      " [ 1.34964832  0.09136813 -1.33021354 -0.47843384 -0.62279503]\n",
      " [-0.78337293  0.41884383 -1.36568547  1.20031187  1.13863902]\n",
      " [-0.28744421  0.76322643 -1.73205081 -0.68474005 -1.22367349]\n",
      " [-1.11679529 -1.14939382 -0.47757318 -1.39602593  1.70063881]\n",
      " [-0.247178   -1.73205081 -1.73205081 -0.69121898 -1.31669878]\n",
      " [-1.13088205 -1.08458758 -0.11063849  0.65406975  1.39594268]]\n",
      "Episode: 19 --- Rewards: [8006209.37332711] --- Steps: 200\n",
      " policy_loss tensor(-0.3898, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "19\n",
      "normalized_state1 [ 0.00214021 -0.02019517 -0.14526468 -1.5489969  -0.07918132]\n",
      "normalized_state2 [ 0.00214021 -0.02019517 -0.14526468 -1.5489969  -0.07918132]\n",
      "self.data_set_X [[ 8.80085281e+01  4.16438939e+01  2.70020587e+01  3.24419863e+01\n",
      "   2.96416060e+01]\n",
      " [-1.41218053e+00 -1.08682651e+00 -5.34993335e-01 -3.57607959e-01\n",
      "   1.34465111e-01]\n",
      " [ 5.90109821e-01 -2.40852810e-01  9.27648230e-01  1.24737085e-01\n",
      "  -1.59397197e+00]\n",
      " [-2.87444208e-01 -1.73205081e+00 -1.73165460e+00 -6.84740054e-01\n",
      "  -1.22367349e+00]\n",
      " [-1.10770356e+00  1.51918403e+00  1.63072635e+00 -6.77282982e-01\n",
      "   1.25568972e+00]\n",
      " [ 5.83876835e-01  1.27483131e+00 -1.05179811e+00  7.37135292e-01\n",
      "  -1.37212681e+00]\n",
      " [-1.30384321e+00 -8.57783430e-01  1.21366299e+00 -1.67584797e+00\n",
      "  -1.19078154e+00]\n",
      " [-1.73205081e+00 -1.73205081e+00 -1.73165460e+00 -1.73205081e+00\n",
      "  -1.22367349e+00]\n",
      " [-8.85194728e-01  6.00545106e-01 -1.42675930e+00 -1.66674540e+00\n",
      "  -9.57955293e-02]\n",
      " [-1.73205081e+00  7.63226434e-01 -1.73165460e+00 -1.73205081e+00\n",
      "  -1.22367349e+00]\n",
      " [-2.87444208e-01  1.13054614e+00 -1.13242098e+00 -6.29427179e-01\n",
      "  -1.22367349e+00]\n",
      " [ 6.44778050e-01 -1.14208137e-01 -1.73205081e+00 -1.73205081e+00\n",
      "  -1.73205081e+00]\n",
      " [-1.71660810e+00  9.64834694e-01  3.01918515e-01 -1.40290797e-01\n",
      "   1.66220522e-01]\n",
      " [ 1.34964832e+00  9.13681340e-02 -1.33021354e+00 -4.78433840e-01\n",
      "  -6.22795035e-01]\n",
      " [-7.83372932e-01  4.18843835e-01 -1.36568547e+00  1.20031187e+00\n",
      "   1.13863902e+00]\n",
      " [-2.87444208e-01  7.63226434e-01 -1.73205081e+00 -6.84740054e-01\n",
      "  -1.22367349e+00]\n",
      " [-1.11679529e+00 -1.14939382e+00 -4.77573181e-01 -1.39602593e+00\n",
      "   1.70063881e+00]\n",
      " [-2.47178005e-01 -1.73205081e+00 -1.73205081e+00 -6.91218982e-01\n",
      "  -1.31669878e+00]\n",
      " [-1.13088205e+00 -1.08458758e+00 -1.10638490e-01  6.54069752e-01\n",
      "   1.39594268e+00]\n",
      " [ 2.14021434e-03 -2.01951734e-02 -1.45264683e-01 -1.54899690e+00\n",
      "  -7.91813204e-02]]\n",
      "Episode: 20 --- Rewards: [7860126.77449579] --- Steps: 200\n",
      " policy_loss tensor(0.3323, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "20\n",
      "normalized_state1 [-0.23451237 -1.11792098 -1.43986501 -0.20628572  0.35540392]\n",
      "normalized_state2 [-0.23451237 -1.11792098 -1.43986501 -0.20628572  0.35540392]\n",
      "self.data_set_X [[ 8.80085281e+01  4.16438939e+01  2.70020587e+01  3.24419863e+01\n",
      "   2.96416060e+01]\n",
      " [-1.41218053e+00 -1.08682651e+00 -5.34993335e-01 -3.57607959e-01\n",
      "   1.34465111e-01]\n",
      " [ 5.90109821e-01 -2.40852810e-01  9.27648230e-01  1.24737085e-01\n",
      "  -1.59397197e+00]\n",
      " [-2.87444208e-01 -1.73205081e+00 -1.73165460e+00 -6.84740054e-01\n",
      "  -1.22367349e+00]\n",
      " [-1.10770356e+00  1.51918403e+00  1.63072635e+00 -6.77282982e-01\n",
      "   1.25568972e+00]\n",
      " [ 5.83876835e-01  1.27483131e+00 -1.05179811e+00  7.37135292e-01\n",
      "  -1.37212681e+00]\n",
      " [-1.30384321e+00 -8.57783430e-01  1.21366299e+00 -1.67584797e+00\n",
      "  -1.19078154e+00]\n",
      " [-1.73205081e+00 -1.73205081e+00 -1.73165460e+00 -1.73205081e+00\n",
      "  -1.22367349e+00]\n",
      " [-8.85194728e-01  6.00545106e-01 -1.42675930e+00 -1.66674540e+00\n",
      "  -9.57955293e-02]\n",
      " [-1.73205081e+00  7.63226434e-01 -1.73165460e+00 -1.73205081e+00\n",
      "  -1.22367349e+00]\n",
      " [-2.87444208e-01  1.13054614e+00 -1.13242098e+00 -6.29427179e-01\n",
      "  -1.22367349e+00]\n",
      " [ 6.44778050e-01 -1.14208137e-01 -1.73205081e+00 -1.73205081e+00\n",
      "  -1.73205081e+00]\n",
      " [-1.71660810e+00  9.64834694e-01  3.01918515e-01 -1.40290797e-01\n",
      "   1.66220522e-01]\n",
      " [ 1.34964832e+00  9.13681340e-02 -1.33021354e+00 -4.78433840e-01\n",
      "  -6.22795035e-01]\n",
      " [-7.83372932e-01  4.18843835e-01 -1.36568547e+00  1.20031187e+00\n",
      "   1.13863902e+00]\n",
      " [-2.87444208e-01  7.63226434e-01 -1.73205081e+00 -6.84740054e-01\n",
      "  -1.22367349e+00]\n",
      " [-1.11679529e+00 -1.14939382e+00 -4.77573181e-01 -1.39602593e+00\n",
      "   1.70063881e+00]\n",
      " [-2.47178005e-01 -1.73205081e+00 -1.73205081e+00 -6.91218982e-01\n",
      "  -1.31669878e+00]\n",
      " [-1.13088205e+00 -1.08458758e+00 -1.10638490e-01  6.54069752e-01\n",
      "   1.39594268e+00]\n",
      " [ 2.14021434e-03 -2.01951734e-02 -1.45264683e-01 -1.54899690e+00\n",
      "  -7.91813204e-02]\n",
      " [-2.34512374e-01 -1.11792098e+00 -1.43986501e+00 -2.06285718e-01\n",
      "   3.55403921e-01]]\n",
      "Episode: 21 --- Rewards: [7806609.22854367] --- Steps: 200\n",
      " policy_loss tensor(-0.0271, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "21\n",
      "normalized_state1 [-1.73205081 -1.73205081 -1.73205081 -0.68503628 -1.73205081]\n",
      "normalized_state2 [-1.73205081 -1.73205081 -1.73205081 -0.68503628 -1.73205081]\n",
      "self.data_set_X [[ 8.80085281e+01  4.16438939e+01  2.70020587e+01  3.24419863e+01\n",
      "   2.96416060e+01]\n",
      " [-1.41218053e+00 -1.08682651e+00 -5.34993335e-01 -3.57607959e-01\n",
      "   1.34465111e-01]\n",
      " [ 5.90109821e-01 -2.40852810e-01  9.27648230e-01  1.24737085e-01\n",
      "  -1.59397197e+00]\n",
      " [-2.87444208e-01 -1.73205081e+00 -1.73165460e+00 -6.84740054e-01\n",
      "  -1.22367349e+00]\n",
      " [-1.10770356e+00  1.51918403e+00  1.63072635e+00 -6.77282982e-01\n",
      "   1.25568972e+00]\n",
      " [ 5.83876835e-01  1.27483131e+00 -1.05179811e+00  7.37135292e-01\n",
      "  -1.37212681e+00]\n",
      " [-1.30384321e+00 -8.57783430e-01  1.21366299e+00 -1.67584797e+00\n",
      "  -1.19078154e+00]\n",
      " [-1.73205081e+00 -1.73205081e+00 -1.73165460e+00 -1.73205081e+00\n",
      "  -1.22367349e+00]\n",
      " [-8.85194728e-01  6.00545106e-01 -1.42675930e+00 -1.66674540e+00\n",
      "  -9.57955293e-02]\n",
      " [-1.73205081e+00  7.63226434e-01 -1.73165460e+00 -1.73205081e+00\n",
      "  -1.22367349e+00]\n",
      " [-2.87444208e-01  1.13054614e+00 -1.13242098e+00 -6.29427179e-01\n",
      "  -1.22367349e+00]\n",
      " [ 6.44778050e-01 -1.14208137e-01 -1.73205081e+00 -1.73205081e+00\n",
      "  -1.73205081e+00]\n",
      " [-1.71660810e+00  9.64834694e-01  3.01918515e-01 -1.40290797e-01\n",
      "   1.66220522e-01]\n",
      " [ 1.34964832e+00  9.13681340e-02 -1.33021354e+00 -4.78433840e-01\n",
      "  -6.22795035e-01]\n",
      " [-7.83372932e-01  4.18843835e-01 -1.36568547e+00  1.20031187e+00\n",
      "   1.13863902e+00]\n",
      " [-2.87444208e-01  7.63226434e-01 -1.73205081e+00 -6.84740054e-01\n",
      "  -1.22367349e+00]\n",
      " [-1.11679529e+00 -1.14939382e+00 -4.77573181e-01 -1.39602593e+00\n",
      "   1.70063881e+00]\n",
      " [-2.47178005e-01 -1.73205081e+00 -1.73205081e+00 -6.91218982e-01\n",
      "  -1.31669878e+00]\n",
      " [-1.13088205e+00 -1.08458758e+00 -1.10638490e-01  6.54069752e-01\n",
      "   1.39594268e+00]\n",
      " [ 2.14021434e-03 -2.01951734e-02 -1.45264683e-01 -1.54899690e+00\n",
      "  -7.91813204e-02]\n",
      " [-2.34512374e-01 -1.11792098e+00 -1.43986501e+00 -2.06285718e-01\n",
      "   3.55403921e-01]\n",
      " [-1.73205081e+00 -1.73205081e+00 -1.73205081e+00 -6.85036278e-01\n",
      "  -1.73205081e+00]]\n",
      "Episode: 22 --- Rewards: [8005330.19125176] --- Steps: 200\n",
      " policy_loss tensor(0.1458, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "22\n",
      "normalized_state1 [-1.65480769 -0.89669828  0.00861783 -0.3917446  -1.325041  ]\n",
      "normalized_state2 [-1.65480769 -0.89669828  0.00861783 -0.3917446  -1.325041  ]\n",
      "self.data_set_X [[ 8.80085281e+01  4.16438939e+01  2.70020587e+01  3.24419863e+01\n",
      "   2.96416060e+01]\n",
      " [-1.41218053e+00 -1.08682651e+00 -5.34993335e-01 -3.57607959e-01\n",
      "   1.34465111e-01]\n",
      " [ 5.90109821e-01 -2.40852810e-01  9.27648230e-01  1.24737085e-01\n",
      "  -1.59397197e+00]\n",
      " [-2.87444208e-01 -1.73205081e+00 -1.73165460e+00 -6.84740054e-01\n",
      "  -1.22367349e+00]\n",
      " [-1.10770356e+00  1.51918403e+00  1.63072635e+00 -6.77282982e-01\n",
      "   1.25568972e+00]\n",
      " [ 5.83876835e-01  1.27483131e+00 -1.05179811e+00  7.37135292e-01\n",
      "  -1.37212681e+00]\n",
      " [-1.30384321e+00 -8.57783430e-01  1.21366299e+00 -1.67584797e+00\n",
      "  -1.19078154e+00]\n",
      " [-1.73205081e+00 -1.73205081e+00 -1.73165460e+00 -1.73205081e+00\n",
      "  -1.22367349e+00]\n",
      " [-8.85194728e-01  6.00545106e-01 -1.42675930e+00 -1.66674540e+00\n",
      "  -9.57955293e-02]\n",
      " [-1.73205081e+00  7.63226434e-01 -1.73165460e+00 -1.73205081e+00\n",
      "  -1.22367349e+00]\n",
      " [-2.87444208e-01  1.13054614e+00 -1.13242098e+00 -6.29427179e-01\n",
      "  -1.22367349e+00]\n",
      " [ 6.44778050e-01 -1.14208137e-01 -1.73205081e+00 -1.73205081e+00\n",
      "  -1.73205081e+00]\n",
      " [-1.71660810e+00  9.64834694e-01  3.01918515e-01 -1.40290797e-01\n",
      "   1.66220522e-01]\n",
      " [ 1.34964832e+00  9.13681340e-02 -1.33021354e+00 -4.78433840e-01\n",
      "  -6.22795035e-01]\n",
      " [-7.83372932e-01  4.18843835e-01 -1.36568547e+00  1.20031187e+00\n",
      "   1.13863902e+00]\n",
      " [-2.87444208e-01  7.63226434e-01 -1.73205081e+00 -6.84740054e-01\n",
      "  -1.22367349e+00]\n",
      " [-1.11679529e+00 -1.14939382e+00 -4.77573181e-01 -1.39602593e+00\n",
      "   1.70063881e+00]\n",
      " [-2.47178005e-01 -1.73205081e+00 -1.73205081e+00 -6.91218982e-01\n",
      "  -1.31669878e+00]\n",
      " [-1.13088205e+00 -1.08458758e+00 -1.10638490e-01  6.54069752e-01\n",
      "   1.39594268e+00]\n",
      " [ 2.14021434e-03 -2.01951734e-02 -1.45264683e-01 -1.54899690e+00\n",
      "  -7.91813204e-02]\n",
      " [-2.34512374e-01 -1.11792098e+00 -1.43986501e+00 -2.06285718e-01\n",
      "   3.55403921e-01]\n",
      " [-1.73205081e+00 -1.73205081e+00 -1.73205081e+00 -6.85036278e-01\n",
      "  -1.73205081e+00]\n",
      " [-1.65480769e+00 -8.96698280e-01  8.61782510e-03 -3.91744596e-01\n",
      "  -1.32504100e+00]]\n",
      "Episode: 23 --- Rewards: [8081378.48786385] --- Steps: 200\n",
      " policy_loss tensor(0.0401, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "23\n",
      "normalized_state1 [-0.06423357 -1.73205081 -1.7316546  -1.73205081 -1.64495241]\n",
      "normalized_state2 [-0.06423357 -1.73205081 -1.7316546  -1.73205081 -1.64495241]\n",
      "self.data_set_X [[ 8.80085281e+01  4.16438939e+01  2.70020587e+01  3.24419863e+01\n",
      "   2.96416060e+01]\n",
      " [-1.41218053e+00 -1.08682651e+00 -5.34993335e-01 -3.57607959e-01\n",
      "   1.34465111e-01]\n",
      " [ 5.90109821e-01 -2.40852810e-01  9.27648230e-01  1.24737085e-01\n",
      "  -1.59397197e+00]\n",
      " [-2.87444208e-01 -1.73205081e+00 -1.73165460e+00 -6.84740054e-01\n",
      "  -1.22367349e+00]\n",
      " [-1.10770356e+00  1.51918403e+00  1.63072635e+00 -6.77282982e-01\n",
      "   1.25568972e+00]\n",
      " [ 5.83876835e-01  1.27483131e+00 -1.05179811e+00  7.37135292e-01\n",
      "  -1.37212681e+00]\n",
      " [-1.30384321e+00 -8.57783430e-01  1.21366299e+00 -1.67584797e+00\n",
      "  -1.19078154e+00]\n",
      " [-1.73205081e+00 -1.73205081e+00 -1.73165460e+00 -1.73205081e+00\n",
      "  -1.22367349e+00]\n",
      " [-8.85194728e-01  6.00545106e-01 -1.42675930e+00 -1.66674540e+00\n",
      "  -9.57955293e-02]\n",
      " [-1.73205081e+00  7.63226434e-01 -1.73165460e+00 -1.73205081e+00\n",
      "  -1.22367349e+00]\n",
      " [-2.87444208e-01  1.13054614e+00 -1.13242098e+00 -6.29427179e-01\n",
      "  -1.22367349e+00]\n",
      " [ 6.44778050e-01 -1.14208137e-01 -1.73205081e+00 -1.73205081e+00\n",
      "  -1.73205081e+00]\n",
      " [-1.71660810e+00  9.64834694e-01  3.01918515e-01 -1.40290797e-01\n",
      "   1.66220522e-01]\n",
      " [ 1.34964832e+00  9.13681340e-02 -1.33021354e+00 -4.78433840e-01\n",
      "  -6.22795035e-01]\n",
      " [-7.83372932e-01  4.18843835e-01 -1.36568547e+00  1.20031187e+00\n",
      "   1.13863902e+00]\n",
      " [-2.87444208e-01  7.63226434e-01 -1.73205081e+00 -6.84740054e-01\n",
      "  -1.22367349e+00]\n",
      " [-1.11679529e+00 -1.14939382e+00 -4.77573181e-01 -1.39602593e+00\n",
      "   1.70063881e+00]\n",
      " [-2.47178005e-01 -1.73205081e+00 -1.73205081e+00 -6.91218982e-01\n",
      "  -1.31669878e+00]\n",
      " [-1.13088205e+00 -1.08458758e+00 -1.10638490e-01  6.54069752e-01\n",
      "   1.39594268e+00]\n",
      " [ 2.14021434e-03 -2.01951734e-02 -1.45264683e-01 -1.54899690e+00\n",
      "  -7.91813204e-02]\n",
      " [-2.34512374e-01 -1.11792098e+00 -1.43986501e+00 -2.06285718e-01\n",
      "   3.55403921e-01]\n",
      " [-1.73205081e+00 -1.73205081e+00 -1.73205081e+00 -6.85036278e-01\n",
      "  -1.73205081e+00]\n",
      " [-1.65480769e+00 -8.96698280e-01  8.61782510e-03 -3.91744596e-01\n",
      "  -1.32504100e+00]\n",
      " [-6.42335702e-02 -1.73205081e+00 -1.73165460e+00 -1.73205081e+00\n",
      "  -1.64495241e+00]]\n",
      "Episode: 24 --- Rewards: [7725520.13994155] --- Steps: 200\n",
      " policy_loss tensor(-0.0751, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "24\n",
      "normalized_state1 [-0.28744421 -0.18927019 -1.7316546  -1.73205081 -1.22367349]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[257], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m problem2 \u001b[39m=\u001b[39m get_problem(\u001b[39m\"\u001b[39m\u001b[39mg4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m algorithm3 \u001b[39m=\u001b[39m MonteCarloGradientPolicyAlgorithm()\n\u001b[0;32m---> 24\u001b[0m res \u001b[39m=\u001b[39m minimize( problem2,\n\u001b[1;32m     25\u001b[0m                 algorithm3,\n\u001b[1;32m     26\u001b[0m                 save_history\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     27\u001b[0m                 termination\u001b[39m=\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mn_iter\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m200\u001b[39;49m),\n\u001b[1;32m     28\u001b[0m                 seed \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m                 return_least_infeasible\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     30\u001b[0m                 verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     32\u001b[0m pf \u001b[39m=\u001b[39m problem2\u001b[39m.\u001b[39mpareto_front()\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPF\u001b[39m\u001b[39m\"\u001b[39m,pf[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/.venv/lib/python3.8/site-packages/pymoo/optimize.py:67\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(problem, algorithm, termination, copy_algorithm, copy_termination, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     algorithm\u001b[39m.\u001b[39msetup(problem, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39m# actually execute the algorithm\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m res \u001b[39m=\u001b[39m algorithm\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     69\u001b[0m \u001b[39m# store the deep copied algorithm in the result object\u001b[39;00m\n\u001b[1;32m     70\u001b[0m res\u001b[39m.\u001b[39malgorithm \u001b[39m=\u001b[39m algorithm\n",
      "File \u001b[0;32m~/.venv/lib/python3.8/site-packages/pymoo/core/algorithm.py:141\u001b[0m, in \u001b[0;36mAlgorithm.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_next():\n\u001b[0;32m--> 141\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/.venv/lib/python3.8/site-packages/pymoo/core/algorithm.py:157\u001b[0m, in \u001b[0;36mAlgorithm.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m     \u001b[39m# get the infill solutions\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     infills \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfill()\n\u001b[1;32m    159\u001b[0m     \u001b[39m# call the advance with them after evaluation\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m infills \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.venv/lib/python3.8/site-packages/pymoo/core/algorithm.py:193\u001b[0m, in \u001b[0;36mAlgorithm.infill\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m     infills \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_infill()\n\u001b[1;32m    191\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[39m# request the infill solutions if the algorithm has implemented it\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     infills \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infill()\n\u001b[1;32m    195\u001b[0m \u001b[39m# set the current generation to the offsprings\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mif\u001b[39;00m infills \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[256], line 100\u001b[0m, in \u001b[0;36mMonteCarloGradientPolicyAlgorithm._infill\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m a, log_p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction(torch\u001b[39m.\u001b[39mTensor(normalized_state)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m     99\u001b[0m log_probs\u001b[39m.\u001b[39mappend(log_p)\n\u001b[0;32m--> 100\u001b[0m new_state, reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(a,normalized_state)\n\u001b[1;32m    101\u001b[0m batch_rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m    102\u001b[0m ep_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[256], line 195\u001b[0m, in \u001b[0;36mMonteCarloGradientPolicyAlgorithm.step\u001b[0;34m(self, action, state)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(is_out_of_bounds_by_problem(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem, [X_new])) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:           \n\u001b[1;32m    194\u001b[0m     X_new \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling\u001b[39m.\u001b[39mdo(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem, \u001b[39m1\u001b[39m, algorithm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 195\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_rewards(current_X, X_new)\n\u001b[1;32m    198\u001b[0m \u001b[39m#if self.is_constraint_model or self.problem.n_eq_constr > 0:\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m#    Y_new = evlaution_of_new_points[0]\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39m#    Constraint_new = evlaution_of_new_points[1]\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m#else:\u001b[39;00m\n\u001b[1;32m    203\u001b[0m new_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(X_new)\n",
      "Cell \u001b[0;32mIn[256], line 160\u001b[0m, in \u001b[0;36mMonteCarloGradientPolicyAlgorithm.get_rewards\u001b[0;34m(self, current_state, new_state)\u001b[0m\n\u001b[1;32m    158\u001b[0m Y_cv \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_constraint_model : \n\u001b[0;32m--> 160\u001b[0m     Y_cv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproblem\u001b[39m.\u001b[39;49mevaluate(new_state, return_values_of\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mCV\u001b[39;49m\u001b[39m\"\u001b[39;49m])    \n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m Y_cv \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(is_out_of_bounds_by_problem(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem, [new_state])) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m3\u001b[39m\u001b[39m*\u001b[39meucli_dist\u001b[39m# + self.penalty_function(new_state) + Y_cv**2)\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.8/site-packages/pymoo/core/problem.py:187\u001b[0m, in \u001b[0;36mProblem.evaluate\u001b[0;34m(self, X, return_values_of, return_as_dictionary, *args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     only_single_value \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(X, \u001b[39mlist\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(X, np\u001b[39m.\u001b[39mndarray))\n\u001b[1;32m    186\u001b[0m \u001b[39m# this is where the actual evaluation takes place\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m _out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo(X, return_values_of, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    189\u001b[0m out \u001b[39m=\u001b[39m {}\n\u001b[1;32m    190\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m _out\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m     \u001b[39m# copy it to a numpy array (it might be one of jax at this point)\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.8/site-packages/pymoo/core/problem.py:229\u001b[0m, in \u001b[0;36mProblem.do\u001b[0;34m(self, X, return_values_of, *args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluate_elementwise(X, out, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    228\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate_vectorized(X, out, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    231\u001b[0m \u001b[39m# finally format the output dictionary\u001b[39;00m\n\u001b[1;32m    232\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_dict(out, \u001b[39mlen\u001b[39m(X), return_values_of)\n",
      "File \u001b[0;32m~/.venv/lib/python3.8/site-packages/pymoo/core/problem.py:237\u001b[0m, in \u001b[0;36mProblem._evaluate_vectorized\u001b[0;34m(self, X, out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_evaluate_vectorized\u001b[39m(\u001b[39mself\u001b[39m, X, out, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 237\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate(X, out, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.8/site-packages/pymoo/problems/single/g.py:125\u001b[0m, in \u001b[0;36mG4._evaluate\u001b[0;34m(self, x, out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m f \u001b[39m=\u001b[39m \u001b[39m5.3578547\u001b[39m \u001b[39m*\u001b[39m x[:, \u001b[39m2\u001b[39m] \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.8356891\u001b[39m \u001b[39m*\u001b[39m x[:, \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m x[:, \u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m37.293239\u001b[39m \u001b[39m*\u001b[39m x[:, \u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m40792.141\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[39m# Constraints\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m u \u001b[39m=\u001b[39m \u001b[39m85.334407\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.0056858\u001b[39;49m \u001b[39m*\u001b[39;49m x[:, \u001b[39m1\u001b[39;49m] \u001b[39m*\u001b[39m x[:, \u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m0.0006262\u001b[39m \u001b[39m*\u001b[39m x[:, \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m x[:, \u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m0.0022053\u001b[39m \u001b[39m*\u001b[39m x[:, \u001b[39m2\u001b[39m] \u001b[39m*\u001b[39m x[:, \u001b[39m4\u001b[39m]\n\u001b[1;32m    126\u001b[0m g1 \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mu\n\u001b[1;32m    127\u001b[0m g2 \u001b[39m=\u001b[39m u \u001b[39m-\u001b[39m \u001b[39m92\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pymoo.problems import get_problem\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.indicators.gd import GD\n",
    "from pymoo.indicators.igd import IGD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pymoo.algorithms.soo.nonconvex.pso import PSO\n",
    "from pymoo.algorithms.soo.nonconvex.ga import GA\n",
    "from pymoo.termination import get_termination\n",
    "from random import randint\n",
    "from pymoo.constraints.as_obj import ConstraintsAsObjective\n",
    "from pymoo.termination.ftol import SingleObjectiveSpaceTermination\n",
    "from pymoo.termination.robust import RobustTermination\n",
    "from pymoo.termination.default import DefaultMultiObjectiveTermination\n",
    "import random\n",
    "\n",
    "torch.manual_seed (1)\n",
    "\n",
    "problem = get_problem(\"ackley\", n_var=20, a=20, b=1/5, c=2 * np.pi)\n",
    "problem1 = get_problem(\"Rastrigin\", n_var=20)\n",
    "problem2 = get_problem(\"Rosenbrock\", n_var=20)\n",
    "problem2 = get_problem(\"g4\")\n",
    "algorithm3 = MonteCarloGradientPolicyAlgorithm()\n",
    "res = minimize( problem2,\n",
    "                algorithm3,\n",
    "                save_history=True,\n",
    "                termination=('n_iter', 200),\n",
    "                seed = 1,\n",
    "                return_least_infeasible=True,\n",
    "                verbose=True)\n",
    "\n",
    "pf = problem2.pareto_front()\n",
    "print(\"PF\",pf[0])\n",
    "ind = GD(pf)\n",
    "print(\"GD\", ind(res.F))\n",
    "ind2 = IGD(pf)\n",
    "print(\"IGD\", ind2(res.F))\n",
    "\n",
    "\n",
    "n_evals = np.array([e.evaluator.n_eval for e in res.history])\n",
    "\n",
    "opt = np.array([e.opt[0].F for e in res.history])\n",
    "print(\"final result X:\",res.X)\n",
    "print(\"final result CV:\",res.CV)\n",
    "print(opt)\n",
    "plt.title(\"Convergence\")\n",
    "plt.plot(n_evals, opt, \"--\")\n",
    "plt.plot(n_evals, np.repeat(pf[0],len(n_evals)), 'k-', lw=1,dashes=[2, 2])\n",
    "#plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[253], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m problem2\u001b[39m.\u001b[39;49mevaluate(res\u001b[39m.\u001b[39;49mX, return_values_of\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mF\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/.venv/lib/python3.8/site-packages/pymoo/core/problem.py:187\u001b[0m, in \u001b[0;36mProblem.evaluate\u001b[0;34m(self, X, return_values_of, return_as_dictionary, *args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     only_single_value \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(X, \u001b[39mlist\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(X, np\u001b[39m.\u001b[39mndarray))\n\u001b[1;32m    186\u001b[0m \u001b[39m# this is where the actual evaluation takes place\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m _out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo(X, return_values_of, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    189\u001b[0m out \u001b[39m=\u001b[39m {}\n\u001b[1;32m    190\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m _out\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m     \u001b[39m# copy it to a numpy array (it might be one of jax at this point)\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.8/site-packages/pymoo/core/problem.py:229\u001b[0m, in \u001b[0;36mProblem.do\u001b[0;34m(self, X, return_values_of, *args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluate_elementwise(X, out, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    228\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate_vectorized(X, out, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    231\u001b[0m \u001b[39m# finally format the output dictionary\u001b[39;00m\n\u001b[1;32m    232\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_dict(out, \u001b[39mlen\u001b[39m(X), return_values_of)\n",
      "File \u001b[0;32m~/.venv/lib/python3.8/site-packages/pymoo/core/problem.py:237\u001b[0m, in \u001b[0;36mProblem._evaluate_vectorized\u001b[0;34m(self, X, out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_evaluate_vectorized\u001b[39m(\u001b[39mself\u001b[39m, X, out, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 237\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate(X, out, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.8/site-packages/pymoo/problems/single/g.py:122\u001b[0m, in \u001b[0;36mG4._evaluate\u001b[0;34m(self, x, out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_evaluate\u001b[39m(\u001b[39mself\u001b[39m, x, out, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 122\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39m5.3578547\u001b[39m \u001b[39m*\u001b[39m x[:, \u001b[39m2\u001b[39;49m] \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.8356891\u001b[39m \u001b[39m*\u001b[39m x[:, \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m x[:, \u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m37.293239\u001b[39m \u001b[39m*\u001b[39m x[:, \u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m40792.141\u001b[39m\n\u001b[1;32m    124\u001b[0m     \u001b[39m# Constraints\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     u \u001b[39m=\u001b[39m \u001b[39m85.334407\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.0056858\u001b[39m \u001b[39m*\u001b[39m x[:, \u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m x[:, \u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m0.0006262\u001b[39m \u001b[39m*\u001b[39m x[:, \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m x[:, \u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m0.0022053\u001b[39m \u001b[39m*\u001b[39m x[:, \u001b[39m2\u001b[39m] \u001b[39m*\u001b[39m x[:, \u001b[39m4\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "problem2.evaluate(res.X, return_values_of=[\"F\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
