{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from pymoo.core.algorithm import Algorithm\n",
    "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
    "from torch.distributions import Normal, Uniform\n",
    "from pymoo.core.initialization import Initialization\n",
    "from pymoo.algorithms.moo.nsga2 import RankAndCrowdingSurvival\n",
    "from pymoo.operators.crossover.sbx import SimulatedBinaryCrossover\n",
    "from pymoo.core.population import Population\n",
    "from pymoo.operators.repair.bounds_repair import is_out_of_bounds_by_problem\n",
    "from pymoo.core.repair import NoRepair\n",
    "from torch import optim\n",
    "import torch\n",
    "import numpy as np\n",
    "from pymoo.util.optimum import filter_optimum\n",
    "from pymoo.problems import get_problem\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.indicators.gd import GD\n",
    "from pymoo.indicators.igd import IGD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pymoo.algorithms.soo.nonconvex.pso import PSO\n",
    "from pymoo.algorithms.soo.nonconvex.ga import GA\n",
    "from pymoo.termination import get_termination\n",
    "from random import randint\n",
    "from pymoo.constraints.as_obj import ConstraintsAsObjective\n",
    "from pymoo.termination.ftol import SingleObjectiveSpaceTermination\n",
    "from pymoo.termination.robust import RobustTermination\n",
    "from pymoo.termination.default import DefaultMultiObjectiveTermination\n",
    "import random\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Deterministic_policy_gradient(Algorithm):\n",
    "    def __init__(self,\n",
    "                 gamma=0.99,\n",
    "                 actor_alpha=0.01,\n",
    "                 critic_alpha=0.01,\n",
    "                 num_rounds=20,\n",
    "                 sampling=FloatRandomSampling(),\n",
    "                 repair=NoRepair(),\n",
    "                 memory_capacity=1000,\n",
    "                 step_size = 0.1,\n",
    "                 var = 2,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.actor_alpha = actor_alpha\n",
    "        self.critic_alpha = critic_alpha\n",
    "        self.gamma = gamma\n",
    "        self.var = var\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.pointer = 0\n",
    "        self.rewards = []\n",
    "        self.steps_taken = []\n",
    "        self.step_size = step_size\n",
    "        self.W_init = tf.random_normal_initializer(mean=0, stddev=0.3)\n",
    "        self.b_init = tf.constant_initializer(0.1)\n",
    "        self.initialization = Initialization(sampling)\n",
    "        self.survival = RankAndCrowdingSurvival()\n",
    "        self.crossover = SimulatedBinaryCrossover(n_offsprings=1)\n",
    "        self.num_rounds = num_rounds\n",
    "        self.repair = repair\n",
    "\n",
    "        \n",
    "    def get_actor(self,input_state_shape, name=''):\n",
    "        tl.layers.set_name_reuse(True)\n",
    "        input_layer = tl.layers.Input(input_state_shape, name='A_input')\n",
    "        layer = tl.layers.Dense(n_units=self.n_var, act=tf.nn.relu, W_init=self.W_init, b_init=self.b_init, name='A_l1')(input_layer)\n",
    "        #print(\"layer1\",layer)\n",
    "        layer = tl.layers.Dense(n_units=self.n_var, act=tf.nn.relu, W_init=self.W_init, b_init=self.b_init, name='A_l2')(layer)\n",
    "        #print(\"layer2\",layer)\n",
    "        layer = tl.layers.Dense(n_units=self.n_var, act=tf.nn.tanh, W_init=self.W_init, b_init=self.b_init, name='A_a')(layer)\n",
    "        #print(\"layer3\",layer)\n",
    "        #layer = tl.layers.Lambda(lambda x: self.step_size * (x+1)-self.step_size)(layer)\n",
    "        #print(\"layer4\",layer)            \n",
    "        return tl.models.Model(inputs=input_layer, outputs=layer)\n",
    "            \n",
    "    def get_critic(self,input_state_shape, input_action_shape, name=''):\n",
    "        tl.layers.set_name_reuse(True)\n",
    "        s = tl.layers.Input(input_state_shape, name='C_s_input')\n",
    "        a = tl.layers.Input(input_action_shape, name='C_a_input')\n",
    "        x = tl.layers.Concat(1)([s, a])\n",
    "        x = tl.layers.Dense(n_units=self.n_var, act=tf.nn.relu, W_init=self.W_init, b_init=self.b_init, name='C_l1')(x)\n",
    "        x = tl.layers.Dense(n_units=self.n_var, act=tf.nn.relu, W_init=self.W_init, b_init=self.b_init, name='C_l2')(x)\n",
    "        x = tl.layers.Dense(n_units=1, W_init=self.W_init, b_init=self.b_init, name='C_out')(x)\n",
    "        return tl.models.Model(inputs=[s, a], outputs=x)\n",
    "            \n",
    "        \n",
    "\n",
    "    def copy_para(self,from_model, to_model):\n",
    "        for i, j in zip(from_model.trainable_weights, to_model.trainable_weights):\n",
    "            j.assign(i)\n",
    "        \n",
    "        \n",
    "\n",
    "    def ema_update(self):\n",
    "        paras = self.actor.trainable_weights + self.critic.trainable_weights    \n",
    "        self.ema.apply(paras)                                               \n",
    "        for i, j in zip(self.actor_target.trainable_weights + self.critic_target.trainable_weights, paras):\n",
    "            i.assign(self.ema.average(j))\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        return self.actor(np.array([s], dtype=np.float32))[0]\n",
    "        \n",
    "    def learn(self):\n",
    "        #print(\"Learning\")\n",
    "        indices = np.random.choice(self.memory_capacity, size=64)    \n",
    "        bt = self.memory[indices, :]                    \n",
    "        bs = bt[:, :self.s_dim]                        \n",
    "        ba = bt[:, self.s_dim:self.s_dim + self.n_var]  \n",
    "        br = bt[:, -self.s_dim - 1:-self.s_dim]        \n",
    "        bs_ = bt[:, -self.s_dim:]                     \n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            a_ = self.actor_target(bs_)\n",
    "            q_ = self.critic_target([bs_, a_])\n",
    "            y = br + self.gamma * q_\n",
    "            q = self.critic([bs, ba])\n",
    "            td_error = tf.losses.mean_squared_error(y, q)\n",
    "        c_grads = tape.gradient(td_error, self.critic.trainable_weights)\n",
    "        self.critic_opt.apply_gradients(zip(c_grads, self.critic.trainable_weights))\n",
    "        self.ema_update()\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        \n",
    "        s = s.astype(np.float32)\n",
    "        s_ = s_.astype(np.float32)\n",
    "\n",
    "        transition = np.hstack((s, a, r, s_))\n",
    "\n",
    "        index = self.pointer % self.memory_capacity\n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        save trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        path = os.path.join('model', '_'.join(['DDPG', 'Pendulum-v0']))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        tl.files.save_weights_to_hdf5(os.path.join(path, 'actor.hdf5'), self.actor)\n",
    "        tl.files.save_weights_to_hdf5(os.path.join(path, 'actor_target.hdf5'), self.actor_target)\n",
    "        tl.files.save_weights_to_hdf5(os.path.join(path, 'critic.hdf5'), self.critic)\n",
    "        tl.files.save_weights_to_hdf5(os.path.join(path, 'critic_target.hdf5'), self.critic_target)\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        load trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        path = os.path.join('model', '_'.join(['DDPG', 'Pendulum-v0']))\n",
    "        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor.hdf5'), self.actor)\n",
    "        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor_target.hdf5'), self.actor_target)\n",
    "        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'critic.hdf5'), self.critic)\n",
    "        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'critic_target.hdf5'), self.critic_target)\n",
    "\n",
    "    def _setup(self, problem, **kwargs):\n",
    "        self.n_var = np.array(self.problem.n_var)\n",
    "        self.s_dim = self.n_var\n",
    "        self.bound = np.array(self.problem.bounds())\n",
    "        self.memory = np.zeros((self.memory_capacity, self.s_dim * 2 + self.n_var + 1), dtype=np.float32)\n",
    "        self.mean = self.problem.xl + (self.problem.xu - self.problem.xl)/2\n",
    "        self.std = np.sqrt(((self.problem.xu - self.problem.xl)**2)/12)\n",
    "\n",
    "        self.actor = self.get_actor([None, self.s_dim])\n",
    "        self.critic = self.get_critic([None, self.s_dim], [None, self.n_var])\n",
    "        self.actor.train()\n",
    "        self.critic.train()\n",
    "        self.actor_target = self.get_actor([None, self.s_dim], name='_target')\n",
    "        self.copy_para(self.actor, self.actor_target)\n",
    "        self.actor_target.eval()\n",
    "\n",
    "        self.critic_target = self.get_critic([None, self.s_dim], [None, self.n_var], name='_target')\n",
    "        self.copy_para(self.critic, self.critic_target)\n",
    "        self.critic_target.eval()\n",
    "\n",
    "        self.R = tl.layers.Input([None, 1], tf.float32, 'r')\n",
    "\n",
    "        self.ema = tf.train.ExponentialMovingAverage(decay=1 - 0.01)  # soft replacement\n",
    "\n",
    "        self.actor_opt = tf.optimizers.Adam(self.actor_alpha)\n",
    "        self.critic_opt = tf.optimizers.Adam(self.critic_alpha)\n",
    "        \n",
    "    def _initialize_infill(self):\n",
    "        return self.initialization.do(self.problem, 1, algorithm=self)\n",
    "\n",
    "    def _initialize_advance(self, infills=None, **kwargs):\n",
    "        self.data_set_X = self.pop.get(\"X\")\n",
    "        #super()._initialize_advance(infills=infills, **kwargs)\n",
    "\n",
    "    def _infill(self):\n",
    "        state = self.get_starting_point()\n",
    "        normalized_state = self.custom_state_normalization(state)\n",
    "        termination_rounds = self.num_rounds \n",
    "        steps = 0\n",
    "        ep_reward = 0\n",
    "\n",
    "        while steps < termination_rounds:\n",
    "            #print(\"self.opt.get(F)\",self.opt.get(\"F\"))\n",
    "           # print(\"self.problem.evaluate(np.array(state))[0]\",self.problem.evaluate(np.array(state))[0])\n",
    "            #print(\"state\",state)\n",
    "            a = self.choose_action(np.array(normalized_state))\n",
    "            a = np.clip(np.random.normal(a, self.var), -1, 1)\n",
    "            action_vector = self.step_size * (a + 1) - self.step_size\n",
    "            new_state, reward = self.step(action_vector,state)\n",
    "            self.store_transition(normalized_state, a, reward / 10, new_state)\n",
    "            if self.pointer > self.memory_capacity:\n",
    "                self.learn()\n",
    "            state = new_state\n",
    "            normalized_state = self.custom_state_normalization(new_state)\n",
    "            ep_reward += reward\n",
    "            steps +=1\n",
    "\n",
    "        #print(\"normalized_state2\",normalized_state)\n",
    "        \n",
    "        self.rewards.append(ep_reward)\n",
    "        self.steps_taken.append(steps)\n",
    "        print(\"Episode: {} --- Rewards: {} --- Steps: {}\".format(self.n_iter, ep_reward, steps))\n",
    "        return self.pop\n",
    "        \n",
    "    def _advance(self, infills=None, **kwargs):\n",
    "        #print(self.pop.get(\"F\"))\n",
    "        #print(self.opt.get(\"F\"))\n",
    "        return super()._advance(infills=infills, **kwargs)\n",
    "    \n",
    "    \n",
    "    def _finalize(self):\n",
    "        state_list = []\n",
    "        for episode in range(200):\n",
    "            state = self.get_starting_point()\n",
    "            episode_reward = 0\n",
    "            for step in range(20):\n",
    "                a = self.choose_action(np.array(state))\n",
    "                action_vector = self.step_size * (a + 1) - self.step_size\n",
    "                #print(\"action_vector\",action_vector)\n",
    "                #print(\"state\",state)\n",
    "                state, reward = self.step(np.array(action_vector), state)\n",
    "                episode_reward += reward\n",
    "                state_list.append(self.problem.evaluate(state, return_values_of=[\"F\"]))   \n",
    "            print(\"Episode: {} --- Rewards: {} --- Steps: {}\".format(episode, episode_reward, 20))\n",
    "        \n",
    "        print(\"final_X:\", state)\n",
    "        print(\"final_F:\", np.min(state_list))\n",
    "        return super()._finalize()\n",
    "    \n",
    "    def get_rewards(self, current_state, new_state):\n",
    "        #print(\"current_state\",current_state)\n",
    "        #print(\"new_state\",new_state)\n",
    "        is_infesible = False\n",
    "        current_state_value = self.problem.evaluate(current_state, return_values_of=[\"F\"])\n",
    "        new_state_value = self.problem.evaluate(new_state, return_values_of=[\"F\"])\n",
    "        diff = new_state_value - current_state_value\n",
    "        eucli_dist = self.euclidean_distance(current_state_value,new_state_value)\n",
    "        c1, c2, c3, c4, c5, c6 = 10, 1, -1, -3, -3, -1\n",
    "        #print(\"eucli_dist:\",eucli_dist)\n",
    "        #print(\"self.problem.evaluate(new_state)[0]\",self.problem.evaluate(new_state)[0])\n",
    "        #print(\"self.problem.evaluate(current_state)[0]\",self.problem.evaluate(current_state)[0])\n",
    "        domain_penalty = self.domain_penalty_function(new_state)\n",
    "        constraint_penalty = self.constraint_penalty_function(new_state)\n",
    "        if domain_penalty > 0 or constraint_penalty > 0:\n",
    "            #print(\"domain_penalty\",domain_penalty)\n",
    "            #print(\"constraint_penalty\",constraint_penalty)\n",
    "            is_infesible = True\n",
    "            #print(\"infesible:\",domain_penalty,constraint_penalty)\n",
    "            return c5*eucli_dist + c6*(domain_penalty + constraint_penalty), is_infesible\n",
    "        elif np.all(diff <= 0):\n",
    "            #print(\"c1:\",diff)\n",
    "            self.data_set_X = np.vstack((self.data_set_X, new_state))\n",
    "            #print(\"self.data_set_X\",self.data_set_X)\n",
    "            off = Population.new(X=self.data_set_X, F = self.problem.evaluate(self.data_set_X, return_values_of=[\"F\"]))\n",
    "            self.pop = off\n",
    "            return c1*eucli_dist, is_infesible\n",
    "        elif np.all(diff >= 0):\n",
    "            #print(\"c4:\",diff)\n",
    "            return c4*eucli_dist, is_infesible\n",
    "        elif np.all(diff == 0):\n",
    "            #print(\"c3:\",diff)\n",
    "            return c3, is_infesible\n",
    "        else:\n",
    "            #print(\"c2:\",diff)\n",
    "            return c2*eucli_dist, is_infesible\n",
    "    \n",
    "    def domain_penalty_function(self, state):\n",
    "        return  np.sum(np.maximum(0, state - self.problem.xu)**2) + np.sum(np.maximum(0, self.problem.xl - state)**2)\n",
    "    \n",
    "    def constraint_penalty_function(self, state):\n",
    "        return np.sum(np.maximum(0, self.problem.evaluate(state, return_values_of=[\"G\"]))**2) + np.sum(np.maximum(0, self.problem.evaluate(state, return_values_of=[\"H\"]))**2)\n",
    "    \n",
    "    def get_starting_point(self):\n",
    "        if self.pop.size <= 2 or np.random.random_sample() > 0.5:\n",
    "            return self.initialization.do(self.problem, 1, algorithm=self).get(\"X\")[0]\n",
    "            while self.constraint_penalty_function(point) > 0:\n",
    "                point = self.initialization.do(self.problem, 1, algorithm=self).get(\"X\")[0]\n",
    "            #print(\"point\",point, self.constraint_penalty_function(point))\n",
    "            print(\"n_point\",point, self.custom_state_normalization(point))\n",
    "            #return point\n",
    "            #return self.initialization.do(self.problem, 1, algorithm=self).get(\"X\")[0]\n",
    "        else:\n",
    "            new_parents = self.survival.do(self.problem, self.pop, n_survive=2)\n",
    "            new_state = self.crossover.do(self.problem, [new_parents]).get(\"X\")[0]\n",
    "            return new_state\n",
    "    \n",
    "    def state_normalization(self, state):\n",
    "        return (state - self.mean)/self.std\n",
    "    \n",
    "    def custom_state_normalization(self, state):\n",
    "        max = self.problem.xu\n",
    "        min = (self.problem.xl + self.problem.xu)/2\n",
    "        return (state-min)/(max-min)\n",
    "    \n",
    "    def euclidean_distance(self, current_state_value, new_state_value):\n",
    "        return np.sum((new_state_value - current_state_value)**2)**0.5\n",
    "\n",
    "    def step(self, action, state):\n",
    "        current_X = state\n",
    "        X_new =  current_X + action\n",
    "        #print(\"current_X:\", current_X)\n",
    "        #print(\"action:\", action)\n",
    "        #print(\"X_new:\", X_new)\n",
    "        reward, is_infesible = self.get_rewards(current_X, X_new)\n",
    "        if is_infesible:\n",
    "            return current_X, reward\n",
    "        #while len(is_out_of_bounds_by_problem(self.problem, [X_new])) > 0:           \n",
    "        #    X_new = self.sampling.do(self.problem, 1, algorithm=self).get(\"X\")[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #if self.is_constraint_model or self.problem.n_eq_constr > 0:\n",
    "        #    Y_new = evlaution_of_new_points[0]\n",
    "        #    Constraint_new = evlaution_of_new_points[1]\n",
    "        #else:\n",
    "        \n",
    "        new_state = np.array(X_new)\n",
    "        #print(\"rewards\",reward)\n",
    "        return new_state, reward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] WARNING: this method is DEPRECATED and has no effect, please remove it from your code.\n",
      "[TL] Input  A_input: [None, array(20)]\n",
      "[TL] Dense  A_l1: 20 relu\n",
      "[TL] Dense  A_l2: 20 relu\n",
      "[TL] Dense  A_a: 20 tanh\n",
      "[TL] WARNING: this method is DEPRECATED and has no effect, please remove it from your code.\n",
      "[TL] Input  C_s_input: [None, array(20)]\n",
      "[TL] Input  C_a_input: [None, array(20)]\n",
      "[TL] Concat concat_179: concat_dim: 1\n",
      "[TL] Dense  C_l1: 20 relu\n",
      "[TL] Dense  C_l2: 20 relu\n",
      "[TL] Dense  C_out: 1 No Activation\n",
      "[TL] WARNING: this method is DEPRECATED and has no effect, please remove it from your code.\n",
      "[TL] Input  A_input: [None, array(20)]\n",
      "[TL] Dense  A_l1: 20 relu\n",
      "[TL] Dense  A_l2: 20 relu\n",
      "[TL] Dense  A_a: 20 tanh\n",
      "[TL] WARNING: this method is DEPRECATED and has no effect, please remove it from your code.\n",
      "[TL] Input  C_s_input: [None, array(20)]\n",
      "[TL] Input  C_a_input: [None, array(20)]\n",
      "[TL] Concat concat_180: concat_dim: 1\n",
      "[TL] Dense  C_l1: 20 relu\n",
      "[TL] Dense  C_l2: 20 relu\n",
      "[TL] Dense  C_out: 1 No Activation\n",
      "[TL] Input  r: [None, 1]\n",
      "Episode: 2 --- Rewards: 5440.316229139692 --- Steps: 20\n",
      "Episode: 3 --- Rewards: 2334.5679938516814 --- Steps: 20\n",
      "Episode: 4 --- Rewards: -3005.914955754526 --- Steps: 20\n",
      "Episode: 5 --- Rewards: 2195.4818675981196 --- Steps: 20\n",
      "Episode: 6 --- Rewards: -1922.7928662011573 --- Steps: 20\n",
      "Episode: 7 --- Rewards: 2566.331496035929 --- Steps: 20\n",
      "Episode: 8 --- Rewards: 571.9927008674863 --- Steps: 20\n",
      "Episode: 9 --- Rewards: -3633.7797375183764 --- Steps: 20\n",
      "Episode: 10 --- Rewards: -697.9458447755092 --- Steps: 20\n",
      "Episode: 11 --- Rewards: -671.7961522848937 --- Steps: 20\n",
      "Episode: 12 --- Rewards: 8257.568306184985 --- Steps: 20\n",
      "Episode: 13 --- Rewards: -1743.9715766788665 --- Steps: 20\n",
      "Episode: 14 --- Rewards: 636.8117522590028 --- Steps: 20\n",
      "Episode: 15 --- Rewards: 732.8812883735118 --- Steps: 20\n",
      "Episode: 16 --- Rewards: 1189.4013794075531 --- Steps: 20\n",
      "Episode: 17 --- Rewards: 1717.6822304159364 --- Steps: 20\n",
      "Episode: 18 --- Rewards: 1208.8484871118444 --- Steps: 20\n",
      "Episode: 19 --- Rewards: 593.892501919066 --- Steps: 20\n",
      "Episode: 20 --- Rewards: -594.4064225867105 --- Steps: 20\n",
      "Episode: 21 --- Rewards: 526.3706640440032 --- Steps: 20\n",
      "Episode: 22 --- Rewards: -636.1759315271614 --- Steps: 20\n",
      "Episode: 23 --- Rewards: 6291.642350690234 --- Steps: 20\n",
      "Episode: 24 --- Rewards: 3503.3879559340344 --- Steps: 20\n",
      "Episode: 25 --- Rewards: -234.49827989478717 --- Steps: 20\n",
      "Episode: 26 --- Rewards: 469.25359451149825 --- Steps: 20\n",
      "Episode: 27 --- Rewards: 4887.593798464035 --- Steps: 20\n",
      "Episode: 28 --- Rewards: 4828.080190789271 --- Steps: 20\n",
      "Episode: 29 --- Rewards: -7238.552877970574 --- Steps: 20\n",
      "Episode: 30 --- Rewards: 11549.448371632367 --- Steps: 20\n",
      "Episode: 31 --- Rewards: -5056.427726895594 --- Steps: 20\n",
      "Episode: 32 --- Rewards: -150.7288304412159 --- Steps: 20\n",
      "Episode: 33 --- Rewards: -396.85402238833365 --- Steps: 20\n",
      "Episode: 34 --- Rewards: 358.1386134116319 --- Steps: 20\n",
      "Episode: 35 --- Rewards: 184.87439713734966 --- Steps: 20\n",
      "Episode: 36 --- Rewards: 1292.7712901282366 --- Steps: 20\n",
      "Episode: 37 --- Rewards: -377.1420551214014 --- Steps: 20\n",
      "Episode: 38 --- Rewards: -155.88752182769122 --- Steps: 20\n",
      "Episode: 39 --- Rewards: 169.6993658112574 --- Steps: 20\n",
      "Episode: 40 --- Rewards: -2355.8917637458812 --- Steps: 20\n",
      "Episode: 41 --- Rewards: 10927.583066189376 --- Steps: 20\n",
      "Episode: 42 --- Rewards: -112.45940139967297 --- Steps: 20\n",
      "Episode: 43 --- Rewards: -540.4516008360156 --- Steps: 20\n",
      "Episode: 44 --- Rewards: 4785.488998198694 --- Steps: 20\n",
      "Episode: 45 --- Rewards: 439.93492153417674 --- Steps: 20\n",
      "Episode: 46 --- Rewards: -733.0076866008667 --- Steps: 20\n",
      "Episode: 47 --- Rewards: -1060.3774526277093 --- Steps: 20\n",
      "Episode: 48 --- Rewards: 5127.9650957041395 --- Steps: 20\n",
      "Episode: 49 --- Rewards: -722.8006592567924 --- Steps: 20\n",
      "Episode: 50 --- Rewards: 4645.159984569405 --- Steps: 20\n",
      "Episode: 51 --- Rewards: -561.6823896539945 --- Steps: 20\n",
      "Episode: 52 --- Rewards: -516.9291489557772 --- Steps: 20\n",
      "Episode: 53 --- Rewards: 181.0145196781864 --- Steps: 20\n",
      "Episode: 54 --- Rewards: 2688.8900010506513 --- Steps: 20\n",
      "Episode: 55 --- Rewards: 572.1085668320393 --- Steps: 20\n",
      "Episode: 56 --- Rewards: 167.8526983273432 --- Steps: 20\n",
      "Episode: 57 --- Rewards: -1002.8478273423716 --- Steps: 20\n",
      "Episode: 58 --- Rewards: -251.76627855786955 --- Steps: 20\n",
      "Episode: 59 --- Rewards: -741.3291525873678 --- Steps: 20\n",
      "Episode: 60 --- Rewards: 12243.690693695027 --- Steps: 20\n",
      "Episode: 61 --- Rewards: 24.355357857774834 --- Steps: 20\n",
      "Episode: 62 --- Rewards: -1698.152097857901 --- Steps: 20\n",
      "Episode: 63 --- Rewards: 87.3284973500954 --- Steps: 20\n",
      "Episode: 64 --- Rewards: -137.26766655322558 --- Steps: 20\n",
      "Episode: 65 --- Rewards: -2070.542109914424 --- Steps: 20\n",
      "Episode: 66 --- Rewards: -2415.528656436284 --- Steps: 20\n",
      "Episode: 67 --- Rewards: 7458.4428306176505 --- Steps: 20\n",
      "Episode: 68 --- Rewards: 3739.906417838276 --- Steps: 20\n",
      "Episode: 69 --- Rewards: 48.74501717912062 --- Steps: 20\n",
      "Episode: 70 --- Rewards: -1599.9714897884423 --- Steps: 20\n",
      "Episode: 71 --- Rewards: 485.3399828718297 --- Steps: 20\n",
      "Episode: 72 --- Rewards: 4969.3810714146075 --- Steps: 20\n",
      "Episode: 73 --- Rewards: 21.836628153508627 --- Steps: 20\n",
      "Episode: 74 --- Rewards: 1026.0774162304915 --- Steps: 20\n",
      "Episode: 75 --- Rewards: -2148.2922944816073 --- Steps: 20\n",
      "Episode: 76 --- Rewards: 5596.202016745117 --- Steps: 20\n",
      "Episode: 77 --- Rewards: -1548.14500155586 --- Steps: 20\n",
      "Episode: 78 --- Rewards: 4276.314748702252 --- Steps: 20\n",
      "Episode: 79 --- Rewards: -466.11885794895 --- Steps: 20\n",
      "Episode: 80 --- Rewards: 3527.9179090829894 --- Steps: 20\n",
      "Episode: 81 --- Rewards: 2031.6724558460173 --- Steps: 20\n",
      "Episode: 82 --- Rewards: -314.4759381626411 --- Steps: 20\n",
      "Episode: 83 --- Rewards: -629.3561638060335 --- Steps: 20\n",
      "Episode: 84 --- Rewards: 2331.8547816638747 --- Steps: 20\n",
      "Episode: 85 --- Rewards: 5891.6816998960785 --- Steps: 20\n",
      "Episode: 86 --- Rewards: -1038.065523513628 --- Steps: 20\n",
      "Episode: 87 --- Rewards: 2645.3946443011155 --- Steps: 20\n",
      "Episode: 88 --- Rewards: 3462.1745869251235 --- Steps: 20\n",
      "Episode: 89 --- Rewards: 5748.643224530677 --- Steps: 20\n",
      "Episode: 90 --- Rewards: 6810.269860665161 --- Steps: 20\n",
      "Episode: 91 --- Rewards: -277.92254640989654 --- Steps: 20\n",
      "Episode: 92 --- Rewards: 5467.257232614902 --- Steps: 20\n",
      "Episode: 93 --- Rewards: 4491.8437119315295 --- Steps: 20\n",
      "Episode: 94 --- Rewards: -2382.820882924918 --- Steps: 20\n",
      "Episode: 95 --- Rewards: -483.14187733249736 --- Steps: 20\n",
      "Episode: 96 --- Rewards: 2049.5442028143343 --- Steps: 20\n",
      "Episode: 97 --- Rewards: -277.96983126504597 --- Steps: 20\n",
      "Episode: 98 --- Rewards: -11.02888310144266 --- Steps: 20\n",
      "Episode: 99 --- Rewards: 1024.1805820837467 --- Steps: 20\n",
      "Episode: 100 --- Rewards: 133.02488579347175 --- Steps: 20\n",
      "Episode: 101 --- Rewards: 4451.60871686057 --- Steps: 20\n",
      "Episode: 102 --- Rewards: 264.7364868237182 --- Steps: 20\n",
      "Episode: 103 --- Rewards: -451.45227583532403 --- Steps: 20\n",
      "Episode: 104 --- Rewards: 3726.2096950101477 --- Steps: 20\n",
      "Episode: 105 --- Rewards: -798.688170417903 --- Steps: 20\n",
      "Episode: 106 --- Rewards: -1206.9012829685407 --- Steps: 20\n",
      "Episode: 107 --- Rewards: -150.61767244900602 --- Steps: 20\n",
      "Episode: 108 --- Rewards: 386.10757676639014 --- Steps: 20\n",
      "Episode: 109 --- Rewards: 951.2464917861776 --- Steps: 20\n",
      "Episode: 110 --- Rewards: -2472.5030214352328 --- Steps: 20\n",
      "Episode: 111 --- Rewards: -168.5706114994714 --- Steps: 20\n",
      "Episode: 112 --- Rewards: -1684.7398578922782 --- Steps: 20\n",
      "Episode: 113 --- Rewards: -930.2771604921991 --- Steps: 20\n",
      "Episode: 114 --- Rewards: 276.2626859575453 --- Steps: 20\n",
      "Episode: 115 --- Rewards: 403.620299933702 --- Steps: 20\n",
      "Episode: 116 --- Rewards: -1139.2295722081258 --- Steps: 20\n",
      "Episode: 117 --- Rewards: 968.8027090063377 --- Steps: 20\n",
      "Episode: 118 --- Rewards: -1443.4404757312846 --- Steps: 20\n",
      "Episode: 119 --- Rewards: 3469.0019600107844 --- Steps: 20\n",
      "Episode: 120 --- Rewards: 12049.625255614394 --- Steps: 20\n",
      "Episode: 121 --- Rewards: 8579.20292782572 --- Steps: 20\n",
      "Episode: 122 --- Rewards: -203.23593779491875 --- Steps: 20\n",
      "Episode: 123 --- Rewards: 9007.434949747876 --- Steps: 20\n",
      "Episode: 124 --- Rewards: 1546.537996430428 --- Steps: 20\n",
      "Episode: 125 --- Rewards: -702.4924999619654 --- Steps: 20\n",
      "Episode: 126 --- Rewards: 514.3754518041656 --- Steps: 20\n",
      "Episode: 127 --- Rewards: -1811.1794278572709 --- Steps: 20\n",
      "Episode: 128 --- Rewards: -896.644603610488 --- Steps: 20\n",
      "Episode: 129 --- Rewards: -2476.955798069377 --- Steps: 20\n",
      "Episode: 130 --- Rewards: -299.6467163691923 --- Steps: 20\n",
      "Episode: 131 --- Rewards: 605.1307229292111 --- Steps: 20\n",
      "Episode: 132 --- Rewards: 5544.982201215138 --- Steps: 20\n",
      "Episode: 133 --- Rewards: -64.06357535932193 --- Steps: 20\n",
      "Episode: 134 --- Rewards: -11.184980155959693 --- Steps: 20\n",
      "Episode: 135 --- Rewards: -871.9300097032165 --- Steps: 20\n",
      "Episode: 136 --- Rewards: 91.69356498189518 --- Steps: 20\n",
      "Episode: 137 --- Rewards: 453.940569970513 --- Steps: 20\n",
      "Episode: 138 --- Rewards: 4321.166926212398 --- Steps: 20\n",
      "Episode: 139 --- Rewards: -163.81035394494393 --- Steps: 20\n",
      "Episode: 140 --- Rewards: 224.95894907699267 --- Steps: 20\n",
      "Episode: 141 --- Rewards: 2216.3251161150374 --- Steps: 20\n",
      "Episode: 142 --- Rewards: -1286.387537994222 --- Steps: 20\n",
      "Episode: 143 --- Rewards: 3129.320068007122 --- Steps: 20\n",
      "Episode: 144 --- Rewards: 5045.148699594205 --- Steps: 20\n",
      "Episode: 145 --- Rewards: -2182.004688063449 --- Steps: 20\n",
      "Episode: 146 --- Rewards: 5880.755335323371 --- Steps: 20\n",
      "Episode: 147 --- Rewards: 10944.49424845067 --- Steps: 20\n",
      "Episode: 148 --- Rewards: 7331.2451007486325 --- Steps: 20\n",
      "Episode: 149 --- Rewards: -152.25677960615076 --- Steps: 20\n",
      "Episode: 150 --- Rewards: 389.08170234004865 --- Steps: 20\n",
      "Episode: 151 --- Rewards: 6483.527125547115 --- Steps: 20\n",
      "Episode: 152 --- Rewards: 1455.0984370013994 --- Steps: 20\n",
      "Episode: 153 --- Rewards: 502.56343713685305 --- Steps: 20\n",
      "Episode: 154 --- Rewards: 371.23481164903205 --- Steps: 20\n",
      "Episode: 155 --- Rewards: 2675.6787045441724 --- Steps: 20\n",
      "Episode: 156 --- Rewards: -753.4293459225983 --- Steps: 20\n",
      "Episode: 157 --- Rewards: 7385.378562713029 --- Steps: 20\n",
      "Episode: 158 --- Rewards: 128.721729059954 --- Steps: 20\n",
      "Episode: 159 --- Rewards: 346.3306133172585 --- Steps: 20\n",
      "Episode: 160 --- Rewards: 414.62672555962695 --- Steps: 20\n",
      "Episode: 161 --- Rewards: 152.17265723295964 --- Steps: 20\n",
      "Episode: 162 --- Rewards: 618.8434816141828 --- Steps: 20\n",
      "Episode: 163 --- Rewards: 11308.084615759011 --- Steps: 20\n",
      "Episode: 164 --- Rewards: 1191.7212783002224 --- Steps: 20\n",
      "Episode: 165 --- Rewards: 19.09921718898613 --- Steps: 20\n",
      "Episode: 166 --- Rewards: -1659.6247757339852 --- Steps: 20\n",
      "Episode: 167 --- Rewards: 6825.246210415076 --- Steps: 20\n",
      "Episode: 168 --- Rewards: 527.7376637179909 --- Steps: 20\n",
      "Episode: 169 --- Rewards: -635.9183537500699 --- Steps: 20\n",
      "Episode: 170 --- Rewards: 1626.9256090616618 --- Steps: 20\n",
      "Episode: 171 --- Rewards: 225.81245385630518 --- Steps: 20\n",
      "Episode: 172 --- Rewards: -84.88590607261449 --- Steps: 20\n",
      "Episode: 173 --- Rewards: -194.53572648685622 --- Steps: 20\n",
      "Episode: 174 --- Rewards: 240.19414731162829 --- Steps: 20\n",
      "Episode: 175 --- Rewards: 1070.856435805401 --- Steps: 20\n",
      "Episode: 176 --- Rewards: 25.390128936163137 --- Steps: 20\n",
      "Episode: 177 --- Rewards: -685.7501966584696 --- Steps: 20\n",
      "Episode: 178 --- Rewards: 1547.0034486094864 --- Steps: 20\n",
      "Episode: 179 --- Rewards: -166.12112710225506 --- Steps: 20\n",
      "Episode: 180 --- Rewards: 1222.8519795665998 --- Steps: 20\n",
      "Episode: 181 --- Rewards: 106.57295531394084 --- Steps: 20\n",
      "Episode: 182 --- Rewards: 10386.259981045445 --- Steps: 20\n",
      "Episode: 183 --- Rewards: -65.44208787538923 --- Steps: 20\n",
      "Episode: 184 --- Rewards: -22.470332338038588 --- Steps: 20\n",
      "Episode: 185 --- Rewards: 1828.061116004379 --- Steps: 20\n",
      "Episode: 186 --- Rewards: -809.0801930557054 --- Steps: 20\n",
      "Episode: 187 --- Rewards: -496.28648617644103 --- Steps: 20\n",
      "Episode: 188 --- Rewards: 237.83758399631233 --- Steps: 20\n",
      "Episode: 189 --- Rewards: 4426.377637243113 --- Steps: 20\n",
      "Episode: 190 --- Rewards: -2413.673621990484 --- Steps: 20\n",
      "Episode: 191 --- Rewards: -322.0411366103858 --- Steps: 20\n",
      "Episode: 192 --- Rewards: -2716.027716273242 --- Steps: 20\n",
      "Episode: 193 --- Rewards: 2458.342237557654 --- Steps: 20\n",
      "Episode: 194 --- Rewards: -1573.7559655609439 --- Steps: 20\n",
      "Episode: 195 --- Rewards: 3396.8195878867455 --- Steps: 20\n",
      "Episode: 196 --- Rewards: 2657.5066862784297 --- Steps: 20\n",
      "Episode: 197 --- Rewards: -515.1036370277948 --- Steps: 20\n",
      "Episode: 198 --- Rewards: -131.4976870380176 --- Steps: 20\n",
      "Episode: 199 --- Rewards: -485.93438565021097 --- Steps: 20\n",
      "Episode: 200 --- Rewards: -303.5396875894071 --- Steps: 20\n",
      "Episode: 0 --- Rewards: -1325.60627773646 --- Steps: 20\n",
      "Episode: 1 --- Rewards: 12605.812328396505 --- Steps: 20\n",
      "Episode: 2 --- Rewards: -1344.7883720483187 --- Steps: 20\n",
      "Episode: 3 --- Rewards: -767.5776651925953 --- Steps: 20\n",
      "Episode: 4 --- Rewards: -1303.6953207791964 --- Steps: 20\n",
      "Episode: 5 --- Rewards: -1118.4172222787256 --- Steps: 20\n",
      "Episode: 6 --- Rewards: 9703.59125438681 --- Steps: 20\n",
      "Episode: 7 --- Rewards: -1317.2659287265105 --- Steps: 20\n",
      "Episode: 8 --- Rewards: -1346.9808363157517 --- Steps: 20\n",
      "Episode: 9 --- Rewards: -4659.155195946551 --- Steps: 20\n",
      "Episode: 10 --- Rewards: -2890.4518347868907 --- Steps: 20\n",
      "Episode: 11 --- Rewards: 15026.974456391035 --- Steps: 20\n",
      "Episode: 12 --- Rewards: 3244.6412462900435 --- Steps: 20\n",
      "Episode: 13 --- Rewards: -1365.9267821331669 --- Steps: 20\n",
      "Episode: 14 --- Rewards: -1268.1912120947245 --- Steps: 20\n",
      "Episode: 15 --- Rewards: -1435.7739956429082 --- Steps: 20\n",
      "Episode: 16 --- Rewards: -3164.5271306175127 --- Steps: 20\n",
      "Episode: 17 --- Rewards: -1314.4607796299981 --- Steps: 20\n",
      "Episode: 18 --- Rewards: -1291.824203322083 --- Steps: 20\n",
      "Episode: 19 --- Rewards: -924.6498111901763 --- Steps: 20\n",
      "Episode: 20 --- Rewards: -1503.5242735019497 --- Steps: 20\n",
      "Episode: 21 --- Rewards: -4613.998359776759 --- Steps: 20\n",
      "Episode: 22 --- Rewards: 2244.0475811259057 --- Steps: 20\n",
      "Episode: 23 --- Rewards: -3411.561840687892 --- Steps: 20\n",
      "Episode: 24 --- Rewards: -1323.9402016848785 --- Steps: 20\n",
      "Episode: 25 --- Rewards: -5788.067140662884 --- Steps: 20\n",
      "Episode: 26 --- Rewards: -1327.70730313314 --- Steps: 20\n",
      "Episode: 27 --- Rewards: -1266.614508568583 --- Steps: 20\n",
      "Episode: 28 --- Rewards: -5784.520830207347 --- Steps: 20\n",
      "Episode: 29 --- Rewards: -1291.070833101956 --- Steps: 20\n",
      "Episode: 30 --- Rewards: -1266.459149137463 --- Steps: 20\n",
      "Episode: 31 --- Rewards: -1365.9267821331669 --- Steps: 20\n",
      "Episode: 32 --- Rewards: -1311.634478535462 --- Steps: 20\n",
      "Episode: 33 --- Rewards: -1331.5139180729393 --- Steps: 20\n",
      "Episode: 34 --- Rewards: 15610.546274175676 --- Steps: 20\n",
      "Episode: 35 --- Rewards: -963.6821675323181 --- Steps: 20\n",
      "Episode: 36 --- Rewards: -2445.0479333908943 --- Steps: 20\n",
      "Episode: 37 --- Rewards: -1320.473543693556 --- Steps: 20\n",
      "Episode: 38 --- Rewards: 1299.27233702854 --- Steps: 20\n",
      "Episode: 39 --- Rewards: 3295.5510558515816 --- Steps: 20\n",
      "Episode: 40 --- Rewards: -1323.5089492260438 --- Steps: 20\n",
      "Episode: 41 --- Rewards: -3592.511777720094 --- Steps: 20\n",
      "Episode: 42 --- Rewards: -1308.1647966092091 --- Steps: 20\n",
      "Episode: 43 --- Rewards: -1326.0480441983589 --- Steps: 20\n",
      "Episode: 44 --- Rewards: -7275.627680862324 --- Steps: 20\n",
      "Episode: 45 --- Rewards: -1321.4124308100215 --- Steps: 20\n",
      "Episode: 46 --- Rewards: -1293.8483880166225 --- Steps: 20\n",
      "Episode: 47 --- Rewards: -3428.883198249959 --- Steps: 20\n",
      "Episode: 48 --- Rewards: -1322.061545704513 --- Steps: 20\n",
      "Episode: 49 --- Rewards: 2044.5881743199584 --- Steps: 20\n",
      "Episode: 50 --- Rewards: -2833.4549121212012 --- Steps: 20\n",
      "Episode: 51 --- Rewards: -1344.7317438363787 --- Steps: 20\n",
      "Episode: 52 --- Rewards: -3153.5075877863032 --- Steps: 20\n",
      "Episode: 53 --- Rewards: -1167.9069899043025 --- Steps: 20\n",
      "Episode: 54 --- Rewards: -1357.1915661682067 --- Steps: 20\n",
      "Episode: 55 --- Rewards: 820.285019552297 --- Steps: 20\n",
      "Episode: 56 --- Rewards: -2679.929175642341 --- Steps: 20\n",
      "Episode: 57 --- Rewards: -3094.3817061870113 --- Steps: 20\n",
      "Episode: 58 --- Rewards: -1365.9267821331669 --- Steps: 20\n",
      "Episode: 59 --- Rewards: -3890.850223634618 --- Steps: 20\n",
      "Episode: 60 --- Rewards: -1349.7632538910648 --- Steps: 20\n",
      "Episode: 61 --- Rewards: -1280.994478558731 --- Steps: 20\n",
      "Episode: 62 --- Rewards: -1320.4886576554097 --- Steps: 20\n",
      "Episode: 63 --- Rewards: -5061.425871278031 --- Steps: 20\n",
      "Episode: 64 --- Rewards: 554.523724171986 --- Steps: 20\n",
      "Episode: 65 --- Rewards: -2384.746952348434 --- Steps: 20\n",
      "Episode: 66 --- Rewards: -1335.1922191133644 --- Steps: 20\n",
      "Episode: 67 --- Rewards: -951.6734302469872 --- Steps: 20\n",
      "Episode: 68 --- Rewards: 7111.745570456433 --- Steps: 20\n",
      "Episode: 69 --- Rewards: 3688.281131400636 --- Steps: 20\n",
      "Episode: 70 --- Rewards: 21837.43034448191 --- Steps: 20\n",
      "Episode: 71 --- Rewards: -1369.3920473747314 --- Steps: 20\n",
      "Episode: 72 --- Rewards: -1258.6709499412102 --- Steps: 20\n",
      "Episode: 73 --- Rewards: -1325.5453772404908 --- Steps: 20\n",
      "Episode: 74 --- Rewards: -1290.4239890118738 --- Steps: 20\n",
      "Episode: 75 --- Rewards: -3604.9236459075037 --- Steps: 20\n",
      "Episode: 76 --- Rewards: -1302.8433378230752 --- Steps: 20\n",
      "Episode: 77 --- Rewards: -1324.9493868958934 --- Steps: 20\n",
      "Episode: 78 --- Rewards: -5393.042284982455 --- Steps: 20\n",
      "Episode: 79 --- Rewards: -1321.2148026813936 --- Steps: 20\n",
      "Episode: 80 --- Rewards: -1259.8709208827504 --- Steps: 20\n",
      "Episode: 81 --- Rewards: -1301.3042453680061 --- Steps: 20\n",
      "Episode: 82 --- Rewards: -2578.585249725519 --- Steps: 20\n",
      "Episode: 83 --- Rewards: -1354.5326726695646 --- Steps: 20\n",
      "Episode: 84 --- Rewards: -669.2603231936228 --- Steps: 20\n",
      "Episode: 85 --- Rewards: -6568.911264025226 --- Steps: 20\n",
      "Episode: 86 --- Rewards: -1321.7985354130765 --- Steps: 20\n",
      "Episode: 87 --- Rewards: -1302.0570118036712 --- Steps: 20\n",
      "Episode: 88 --- Rewards: -1304.9783818559408 --- Steps: 20\n",
      "Episode: 89 --- Rewards: 1144.2081708250707 --- Steps: 20\n",
      "Episode: 90 --- Rewards: -1287.4865907595772 --- Steps: 20\n",
      "Episode: 91 --- Rewards: -22.769893358500326 --- Steps: 20\n",
      "Episode: 92 --- Rewards: 8986.820778859183 --- Steps: 20\n",
      "Episode: 93 --- Rewards: -1302.8824274180743 --- Steps: 20\n",
      "Episode: 94 --- Rewards: -4985.4844667715315 --- Steps: 20\n",
      "Episode: 95 --- Rewards: -1359.4004708973976 --- Steps: 20\n",
      "Episode: 96 --- Rewards: -1286.886978857249 --- Steps: 20\n",
      "Episode: 97 --- Rewards: 76.59994192747331 --- Steps: 20\n",
      "Episode: 98 --- Rewards: -1230.0693049025865 --- Steps: 20\n",
      "Episode: 99 --- Rewards: -1271.854114591963 --- Steps: 20\n",
      "Episode: 100 --- Rewards: -1279.282544520613 --- Steps: 20\n",
      "Episode: 101 --- Rewards: -1350.5278572268733 --- Steps: 20\n",
      "Episode: 102 --- Rewards: -1341.2278390267877 --- Steps: 20\n",
      "Episode: 103 --- Rewards: -5002.751427982955 --- Steps: 20\n",
      "Episode: 104 --- Rewards: -1273.1236682317758 --- Steps: 20\n",
      "Episode: 105 --- Rewards: -1285.8429004853726 --- Steps: 20\n",
      "Episode: 106 --- Rewards: -1350.4498087697266 --- Steps: 20\n",
      "Episode: 107 --- Rewards: 392.23227103213827 --- Steps: 20\n",
      "Episode: 108 --- Rewards: -1340.5378540468334 --- Steps: 20\n",
      "Episode: 109 --- Rewards: -3123.3381634070174 --- Steps: 20\n",
      "Episode: 110 --- Rewards: -1367.2305880133204 --- Steps: 20\n",
      "Episode: 111 --- Rewards: -1275.7693040322156 --- Steps: 20\n",
      "Episode: 112 --- Rewards: -1365.9267821331669 --- Steps: 20\n",
      "Episode: 113 --- Rewards: -1290.877694054327 --- Steps: 20\n",
      "Episode: 114 --- Rewards: 3118.6307281899553 --- Steps: 20\n",
      "Episode: 115 --- Rewards: -1328.9893539642298 --- Steps: 20\n",
      "Episode: 116 --- Rewards: -1301.5877737080739 --- Steps: 20\n",
      "Episode: 117 --- Rewards: -885.1370896596484 --- Steps: 20\n",
      "Episode: 118 --- Rewards: -1032.0254833166591 --- Steps: 20\n",
      "Episode: 119 --- Rewards: -1301.5670994814102 --- Steps: 20\n",
      "Episode: 120 --- Rewards: 11910.603247795898 --- Steps: 20\n",
      "Episode: 121 --- Rewards: -668.7313352652563 --- Steps: 20\n",
      "Episode: 122 --- Rewards: 178.49622163926142 --- Steps: 20\n",
      "Episode: 123 --- Rewards: -1265.2107258189435 --- Steps: 20\n",
      "Episode: 124 --- Rewards: -1793.4849830640524 --- Steps: 20\n",
      "Episode: 125 --- Rewards: -3392.0181839991706 --- Steps: 20\n",
      "Episode: 126 --- Rewards: -1306.3006181023359 --- Steps: 20\n",
      "Episode: 127 --- Rewards: -1184.5342777361152 --- Steps: 20\n",
      "Episode: 128 --- Rewards: 866.8622284083976 --- Steps: 20\n",
      "Episode: 129 --- Rewards: -1289.0246013208368 --- Steps: 20\n",
      "Episode: 130 --- Rewards: -1268.1912120947245 --- Steps: 20\n",
      "Episode: 131 --- Rewards: -1026.2340893973249 --- Steps: 20\n",
      "Episode: 132 --- Rewards: 3650.1854189750666 --- Steps: 20\n",
      "Episode: 133 --- Rewards: -1268.1912120947245 --- Steps: 20\n",
      "Episode: 134 --- Rewards: 2352.233864702075 --- Steps: 20\n",
      "Episode: 135 --- Rewards: -1266.9629951650713 --- Steps: 20\n",
      "Episode: 136 --- Rewards: -1333.8213983988057 --- Steps: 20\n",
      "Episode: 137 --- Rewards: -1330.946587695171 --- Steps: 20\n",
      "Episode: 138 --- Rewards: -1363.9254021716104 --- Steps: 20\n",
      "Episode: 139 --- Rewards: -1365.9267821331669 --- Steps: 20\n",
      "Episode: 140 --- Rewards: -4316.532668370288 --- Steps: 20\n",
      "Episode: 141 --- Rewards: -4039.9413540260703 --- Steps: 20\n",
      "Episode: 142 --- Rewards: -1314.5709031476283 --- Steps: 20\n",
      "Episode: 143 --- Rewards: -3045.193247354892 --- Steps: 20\n",
      "Episode: 144 --- Rewards: -1328.7981677826435 --- Steps: 20\n",
      "Episode: 145 --- Rewards: -2709.7565159065734 --- Steps: 20\n",
      "Episode: 146 --- Rewards: -1330.3988722985202 --- Steps: 20\n",
      "Episode: 147 --- Rewards: -1349.9417949716783 --- Steps: 20\n",
      "Episode: 148 --- Rewards: -7415.894354649227 --- Steps: 20\n",
      "Episode: 149 --- Rewards: -1343.0115503039528 --- Steps: 20\n",
      "Episode: 150 --- Rewards: -3718.1962776512337 --- Steps: 20\n",
      "Episode: 151 --- Rewards: -7773.515211412161 --- Steps: 20\n",
      "Episode: 152 --- Rewards: 528.9419402573058 --- Steps: 20\n",
      "Episode: 153 --- Rewards: -2967.0915458055806 --- Steps: 20\n",
      "Episode: 154 --- Rewards: -1313.3710580255809 --- Steps: 20\n",
      "Episode: 155 --- Rewards: -1322.8982884181432 --- Steps: 20\n",
      "Episode: 156 --- Rewards: 2743.267779953003 --- Steps: 20\n",
      "Episode: 157 --- Rewards: -1365.9267821331669 --- Steps: 20\n",
      "Episode: 158 --- Rewards: -2043.7499641939667 --- Steps: 20\n",
      "Episode: 159 --- Rewards: 6153.835695949856 --- Steps: 20\n",
      "Episode: 160 --- Rewards: 2607.2185756187155 --- Steps: 20\n",
      "Episode: 161 --- Rewards: -1365.9267821331669 --- Steps: 20\n",
      "Episode: 162 --- Rewards: -1308.3377976477607 --- Steps: 20\n",
      "Episode: 163 --- Rewards: -1102.7867569868517 --- Steps: 20\n",
      "Episode: 164 --- Rewards: -3089.679098269643 --- Steps: 20\n",
      "Episode: 165 --- Rewards: -1291.0676286649816 --- Steps: 20\n",
      "Episode: 166 --- Rewards: 7959.207887301682 --- Steps: 20\n",
      "Episode: 167 --- Rewards: 7609.44302339688 --- Steps: 20\n",
      "Episode: 168 --- Rewards: -8740.033563952315 --- Steps: 20\n",
      "Episode: 169 --- Rewards: -4705.12691936038 --- Steps: 20\n",
      "Episode: 170 --- Rewards: -2785.3972284188685 --- Steps: 20\n",
      "Episode: 171 --- Rewards: -2195.523290486154 --- Steps: 20\n",
      "Episode: 172 --- Rewards: -1305.1101272654514 --- Steps: 20\n",
      "Episode: 173 --- Rewards: -5297.574834223069 --- Steps: 20\n",
      "Episode: 174 --- Rewards: -1346.204944586315 --- Steps: 20\n",
      "Episode: 175 --- Rewards: -1305.8801278712199 --- Steps: 20\n",
      "Episode: 176 --- Rewards: -4924.960255293934 --- Steps: 20\n",
      "Episode: 177 --- Rewards: 1084.977234535243 --- Steps: 20\n",
      "Episode: 178 --- Rewards: -1268.2855480420183 --- Steps: 20\n",
      "Episode: 179 --- Rewards: -1354.370681900612 --- Steps: 20\n",
      "Episode: 180 --- Rewards: -898.6778736721158 --- Steps: 20\n",
      "Episode: 181 --- Rewards: -1349.563110755374 --- Steps: 20\n",
      "Episode: 182 --- Rewards: 951.2836979593385 --- Steps: 20\n",
      "Episode: 183 --- Rewards: -4252.774187350855 --- Steps: 20\n",
      "Episode: 184 --- Rewards: -1339.0385578017272 --- Steps: 20\n",
      "Episode: 185 --- Rewards: -1664.2071544366743 --- Steps: 20\n",
      "Episode: 186 --- Rewards: -2873.430062511482 --- Steps: 20\n",
      "Episode: 187 --- Rewards: 35506.42761443301 --- Steps: 20\n",
      "Episode: 188 --- Rewards: 8303.548444106418 --- Steps: 20\n",
      "Episode: 189 --- Rewards: 569.2587268303578 --- Steps: 20\n",
      "Episode: 190 --- Rewards: -1268.5112244210413 --- Steps: 20\n",
      "Episode: 191 --- Rewards: -1303.1792335641105 --- Steps: 20\n",
      "Episode: 192 --- Rewards: -1358.356700967642 --- Steps: 20\n",
      "Episode: 193 --- Rewards: -5545.031167223418 --- Steps: 20\n",
      "Episode: 194 --- Rewards: -1744.5096545911229 --- Steps: 20\n",
      "Episode: 195 --- Rewards: 1023.4163454342324 --- Steps: 20\n",
      "Episode: 196 --- Rewards: -408.16210497061456 --- Steps: 20\n",
      "Episode: 197 --- Rewards: -1262.0125734458563 --- Steps: 20\n",
      "Episode: 198 --- Rewards: -1475.7209684181344 --- Steps: 20\n",
      "Episode: 199 --- Rewards: -1282.9306888020276 --- Steps: 20\n",
      "final_X: [-0.1540487  -0.09907465  0.85273829  0.48287789  0.34967595 -0.22540345\n",
      "  0.79816346 -0.32908341 -0.03107908  1.68469472  1.46830021  0.21212714\n",
      " -0.47718028 -1.06901058  0.71163557  1.2753926   1.43976079  1.5749865\n",
      " -0.38971779 -0.63808864]\n",
      "final_F: 1818.6404640776082\n",
      "PF [0.]\n",
      "GD 1840.4835300793156\n",
      "IGD 1840.4835300793156\n",
      "final result X: [-0.31633747 -0.26466935  1.01493365  0.55701552  0.25031413 -0.14820874\n",
      "  0.66267291 -0.45571721  0.04889728  1.59181345  1.33104707  0.32185215\n",
      " -0.55212014 -1.08136185  0.82670495  1.16001019  1.49593366  1.49444154\n",
      " -0.22951049 -0.47269336]\n",
      "final result F: [1840.48353008]\n",
      "final result CV: [0.]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tf.random.set_seed (2)\n",
    "\n",
    "problem = get_problem(\"ackley\", n_var=20, a=20, b=1/5, c=2 * np.pi)\n",
    "problem1 = get_problem(\"Rastrigin\", n_var=20)\n",
    "problem2 = get_problem(\"Rosenbrock\", n_var=20)\n",
    "problem3 = get_problem(\"g6\")\n",
    "problem4 = get_problem(\"dtlz3\")\n",
    "algorithm3 = Deep_Deterministic_policy_gradient(step_size= 0.01, var=1,num_rounds=20)\n",
    "res = minimize( problem2,\n",
    "                algorithm3,\n",
    "                save_history=False,\n",
    "                termination=('n_iter', 200),\n",
    "                seed = 2,\n",
    "                return_least_infeasible=True,\n",
    "                verbose=True)\n",
    "\n",
    "pf = problem2.pareto_front()\n",
    "print(\"PF\",pf[0])\n",
    "ind = GD(pf)\n",
    "print(\"GD\", ind(res.F))\n",
    "ind2 = IGD(pf)\n",
    "print(\"IGD\", ind2(res.F))\n",
    "\n",
    "\n",
    "n_evals = np.array([e.evaluator.n_eval for e in res.history])\n",
    "\n",
    "opt = np.array([e.opt[0].F for e in res.history])\n",
    "print(\"final result X:\",res.X)\n",
    "print(\"final result F:\",res.F)\n",
    "print(\"final result CV:\",res.CV)\n",
    "print(opt)\n",
    "plt.title(\"Convergence\")\n",
    "plt.plot(n_evals, opt, \"--\")\n",
    "plt.plot(n_evals, np.repeat(pf[0],len(n_evals)), 'k-', lw=1,dashes=[2, 2])\n",
    "#plt.yscale(\"log\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3879.06268577]), array([], dtype=float64), array([], dtype=float64))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem2.evaluate(res.X, return_values_of=[\"F\", \"H\", \"G\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06462775510972782"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.maximum(0, res.X)) + np.sum(np.maximum(0, problem2.evaluate(res.X, return_values_of=[\"H\"]))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
