{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Policy(nn.Module):\n",
    "  def __init__(self,input_shape):\n",
    "    super().__init__()\n",
    "    print(\"input_shape\",input_shape)\n",
    "    self.model = nn.Sequential(\n",
    "        nn.Linear(input_shape[0],64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32,input_shape[0]),\n",
    "    )\n",
    "  def forward(self,x):\n",
    "    return self.model(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.core.algorithm import Algorithm\n",
    "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
    "from torch.distributions import Categorical,Normal\n",
    "from pymoo.core.initialization import Initialization\n",
    "from pymoo.core.population import Population\n",
    "from pymoo.core.repair import NoRepair\n",
    "from torch import optim\n",
    "import torch\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class MonteCarloGradientPolicyAlgorithm(Algorithm):\n",
    "    def __init__(self,\n",
    "                 gamma=0.99,\n",
    "                 alpha=0.01,\n",
    "                 num_rounds=100,\n",
    "                 sample_size=500,\n",
    "                 sampling=FloatRandomSampling(),\n",
    "                 repair=NoRepair(),\n",
    "                 **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : \n",
    "            The environment to be used in the algorithm.\n",
    "        policy : {Policy}\n",
    "            The policy to be used in the algorithm.\n",
    "        gamma : float, optional\n",
    "            The discount factor used in the algorithm. The default is 0.99.\n",
    "        alpha : float, optional\n",
    "            The learning rate used in the algorithm. The default is 0.01.\n",
    "        num_episodes : int, optional\n",
    "            The number of episodes to be run in the algorithm. The default is 100.\n",
    "        sample_size : int, optional\n",
    "            The number of samples to be generated from the problems and used in the acquisition function. \n",
    "            The default is 10.\n",
    "        sampling : {Sampling}, optional\n",
    "            The sampling method used to generate the initial samples. The default is FloatRandomSampling().\n",
    "        \"\"\"\n",
    "         \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.num_rounds = num_rounds\n",
    "        self.sample_size = sample_size\n",
    "        self.sampling = sampling\n",
    "        self.repair = repair\n",
    "        \n",
    "        self.initialization = Initialization(sampling)\n",
    "        self.is_constraint_model = False\n",
    "        self.optimizer = None\n",
    "        self.data_set_X = None\n",
    "        self.model = None\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.rewards = []\n",
    "        self.steps_taken = []\n",
    "\n",
    "    def _setup(self, problem, **kwargs):\n",
    "        self.is_constraint_model = False\n",
    "        self.model = Policy(np.array([self.problem.n_var]))\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 1e-2)\n",
    "        \n",
    "    def _initialize_infill(self):\n",
    "        return self.initialization.do(self.problem, 1, algorithm=self)\n",
    "\n",
    "    def _initialize_advance(self, infills=None, **kwargs):\n",
    "        self.data_set_X = self.pop.get(\"X\")\n",
    "        #super()._initialize_advance(infills=infills, **kwargs)\n",
    "\n",
    "    def _infill(self):\n",
    "        state = self.sampling.do(self.problem, 1, algorithm=self).get(\"X\")[0]\n",
    "        steps = 0\n",
    "        ep_rewards = 0\n",
    "        batch_rewards = []\n",
    "        log_probs = []\n",
    "        \n",
    "        while self.opt.get(\"F\")[0][0] < self.problem.evaluate(np.array(state))[0] or steps < 50:\n",
    "            #print(\"self.opt.get(F)\",self.opt.get(\"F\"))\n",
    "           # print(\"self.problem.evaluate(np.array(state))[0]\",self.problem.evaluate(np.array(state))[0])\n",
    "            a, log_p = self.action(self.model, torch.Tensor(state).unsqueeze(0))\n",
    "            log_probs.append(log_p)\n",
    "            new_state, reward = self.step(a,state)\n",
    "            batch_rewards.append(reward)\n",
    "            ep_rewards += reward\n",
    "            steps +=1 \n",
    "            #print(\"state:\", state)\n",
    "            state = new_state\n",
    "      \n",
    "        \n",
    "        self.rewards.append(ep_rewards)\n",
    "        self.steps_taken.append(steps)\n",
    "        print(\"Episode: {} --- Rewards: {} --- Steps: {}\".format(self.n_iter, ep_rewards, steps))\n",
    "        self.update_policy(self.n_iter, self.optimizer, batch_rewards, log_probs)\n",
    "\n",
    "        \n",
    "        return self.pop\n",
    "        \n",
    "    def _advance(self, infills=None, **kwargs):\n",
    "        return super()._advance(infills=infills, **kwargs)\n",
    "    \n",
    "    \n",
    "    def _finalize(self):\n",
    "        return super()._finalize()\n",
    "    \n",
    "    def action(self, model, s):\n",
    "        # simple pytorch aproach for action-selection and log-prob calc \n",
    "        action_parameters = model(s)\n",
    "        #print(\"action_parameters:\", action_parameters)\n",
    "        #m = Categorical(prob)\n",
    "        #print(\"action_parameters[:, :1]:\", action_parameters[:, :1])\n",
    "        #print(\"action_parameters[:, 1:]:\", action_parameters[:, 1:])\n",
    "        #mu, sigma = action_parameters[:, :1], torch.exp(action_parameters[:, 1:])\n",
    "        mu, c = action_parameters[:, :1], 1\n",
    "        #print(\"mu:\", mu)\n",
    "        \n",
    "        m = Normal(action_parameters, torch.Tensor([[1,1]]))\n",
    "        #print(\"m:\", m)\n",
    "        a = m.sample()\n",
    "        #print(\"a:\", a.tolist())\n",
    "        # log p(a∣π(s))\n",
    "        log_p = m.log_prob(a)\n",
    "        #print(\"log_p:\", log_p)\n",
    "        #print(a.item(), log_p)\n",
    "        return a.tolist(), log_p\n",
    "    \n",
    "    def step(self, action: np.ndarray, state):\n",
    "        current_X = state\n",
    "        X_new = current_X[0] + action[0]\n",
    "        xl, xu = self.problem.bounds()\n",
    "        reward = 0\n",
    "        #print(\"current_X:\", current_X)\n",
    "        #print(\"action:\", action)\n",
    "        #print(\"X_new:\", X_new)\n",
    "        if np.any(X_new < xl) or np.any(X_new > xu):\n",
    "            reward = -(max(max(xl - X_new), max(X_new - xu)))\n",
    "            X_new = self.sampling.do(self.problem, 1, algorithm=self).get(\"X\")[0]\n",
    "        else:\n",
    "            evlaution_of_new_points = self.problem.evaluate(np.array(X_new))\n",
    "            Y_new = evlaution_of_new_points[0]    \n",
    "            if Y_new < self.opt.get(\"F\"):\n",
    "                reward = self.opt.get(\"F\")[0][0] - Y_new\n",
    "                self.data_set_X = np.vstack((self.data_set_X, X_new))\n",
    "                off = Population.new(X=self.data_set_X)\n",
    "                self.pop = off\n",
    "                self.repair(self.problem, off)\n",
    "\n",
    "        \n",
    "        \n",
    "        #if self.is_constraint_model or self.problem.n_eq_constr > 0:\n",
    "        #    Y_new = evlaution_of_new_points[0]\n",
    "        #    Constraint_new = evlaution_of_new_points[1]\n",
    "        #else:\n",
    "        \n",
    "        state = np.array(X_new)\n",
    "        #print(\"rewards\",reward)\n",
    "        return state, reward\n",
    "    \n",
    "    def update_policy(self, ep, optimizer,batch_rewards,log_probs):\n",
    "        R = 0\n",
    "        gamma = 0.99\n",
    "        policy_loss = []\n",
    "        rewards = []\n",
    "        #calc discounted Rewards\n",
    "        for r in batch_rewards[::-1]: # reverses the list of rewards \n",
    "            R = r + gamma * R\n",
    "            rewards.insert(0, R) # inserts the current reward to first position\n",
    "        \n",
    "        #print(\"rewards\",rewards)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        # standardization to get data of zero mean and varianz 1, stabilizes learning \n",
    "        #-- attention scaling rewards looses information of special events with higher rewards - addapting on different environments  \n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + ep)\n",
    "        for log_prob, reward in zip(log_probs, rewards):\n",
    "            policy_loss.append(-log_prob * reward) #baseline+\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \"\"\"G = 0\n",
    "        for t in reversed(range(len(states))):\n",
    "            G = self.gamma * G + rewards[t]\n",
    "            state = states[t]\n",
    "            action = actions[t]\n",
    "            grad_log_prob = self.policy.grad_log_prob(state, action)\n",
    "            self.policy.theta += self.alpha * G * grad_log_prob\"\"\"\n",
    "\n",
    "    \"\"\"def run_episode(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.policy.sample_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        return states, actions, rewards\"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape [2]\n",
      "Episode: 2 --- Rewards: 1.6523791353335362 --- Steps: 12\n",
      "Episode: 3 --- Rewards: 1.9120848794820198 --- Steps: 5\n",
      "Episode: 4 --- Rewards: 6.586166769446875 --- Steps: 6\n",
      "Episode: 5 --- Rewards: 0 --- Steps: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m problem1 \u001b[39m=\u001b[39m get_problem(\u001b[39m\"\u001b[39m\u001b[39mGriewank\u001b[39m\u001b[39m\"\u001b[39m, n_var\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     21\u001b[0m algorithm3 \u001b[39m=\u001b[39m MonteCarloGradientPolicyAlgorithm()\n\u001b[0;32m---> 22\u001b[0m res \u001b[39m=\u001b[39m minimize( problem1,\n\u001b[1;32m     23\u001b[0m                 algorithm3,\n\u001b[1;32m     24\u001b[0m                 save_history\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     25\u001b[0m                 termination\u001b[39m=\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mn_iter\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m5\u001b[39;49m),\n\u001b[1;32m     26\u001b[0m                 seed \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     27\u001b[0m                 return_least_infeasible\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     28\u001b[0m                 verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     30\u001b[0m pf \u001b[39m=\u001b[39m problem\u001b[39m.\u001b[39mpareto_front()\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPF\u001b[39m\u001b[39m\"\u001b[39m,pf[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pymoo/optimize.py:67\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(problem, algorithm, termination, copy_algorithm, copy_termination, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     algorithm\u001b[39m.\u001b[39msetup(problem, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39m# actually execute the algorithm\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m res \u001b[39m=\u001b[39m algorithm\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     69\u001b[0m \u001b[39m# store the deep copied algorithm in the result object\u001b[39;00m\n\u001b[1;32m     70\u001b[0m res\u001b[39m.\u001b[39malgorithm \u001b[39m=\u001b[39m algorithm\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pymoo/core/algorithm.py:141\u001b[0m, in \u001b[0;36mAlgorithm.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_next():\n\u001b[0;32m--> 141\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pymoo/core/algorithm.py:157\u001b[0m, in \u001b[0;36mAlgorithm.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m     \u001b[39m# get the infill solutions\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     infills \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfill()\n\u001b[1;32m    159\u001b[0m     \u001b[39m# call the advance with them after evaluation\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m infills \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pymoo/core/algorithm.py:193\u001b[0m, in \u001b[0;36mAlgorithm.infill\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m     infills \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_infill()\n\u001b[1;32m    191\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[39m# request the infill solutions if the algorithm has implemented it\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     infills \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infill()\n\u001b[1;32m    195\u001b[0m \u001b[39m# set the current generation to the offsprings\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mif\u001b[39;00m infills \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[54], line 97\u001b[0m, in \u001b[0;36mMonteCarloGradientPolicyAlgorithm._infill\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps_taken\u001b[39m.\u001b[39mappend(steps)\n\u001b[1;32m     96\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpisode: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m --- Rewards: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m --- Steps: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter, ep_rewards, steps))\n\u001b[0;32m---> 97\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_policy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, batch_rewards, log_probs)\n\u001b[1;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpop\n",
      "Cell \u001b[0;32mIn[54], line 181\u001b[0m, in \u001b[0;36mMonteCarloGradientPolicyAlgorithm.update_policy\u001b[0;34m(self, ep, optimizer, batch_rewards, log_probs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     policy_loss\u001b[39m.\u001b[39mappend(\u001b[39m-\u001b[39mlog_prob \u001b[39m*\u001b[39m reward) \u001b[39m#baseline+\u001b[39;00m\n\u001b[1;32m    180\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 181\u001b[0m policy_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(policy_loss)\u001b[39m.\u001b[39msum()\n\u001b[1;32m    182\u001b[0m policy_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    183\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "from pymoo.problems import get_problem\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.indicators.gd import GD\n",
    "from pymoo.indicators.igd import IGD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pymoo.algorithms.soo.nonconvex.pso import PSO\n",
    "from pymoo.algorithms.soo.nonconvex.ga import GA\n",
    "from pymoo.termination import get_termination\n",
    "from random import randint\n",
    "from pymoo.constraints.as_obj import ConstraintsAsObjective\n",
    "from pymoo.termination.ftol import SingleObjectiveSpaceTermination\n",
    "from pymoo.termination.robust import RobustTermination\n",
    "from pymoo.termination.default import DefaultMultiObjectiveTermination\n",
    "import random\n",
    "\n",
    "torch.manual_seed (0)\n",
    "\n",
    "problem = get_problem(\"ackley\", n_var=2, a=20, b=1/5, c=2 * np.pi)\n",
    "problem1 = get_problem(\"Griewank\", n_var=2)\n",
    "algorithm3 = MonteCarloGradientPolicyAlgorithm()\n",
    "res = minimize( problem1,\n",
    "                algorithm3,\n",
    "                save_history=True,\n",
    "                termination=('n_iter', 5),\n",
    "                seed = 1,\n",
    "                return_least_infeasible=True,\n",
    "                verbose=True)\n",
    "\n",
    "pf = problem.pareto_front()\n",
    "print(\"PF\",pf[0])\n",
    "ind = GD(pf)\n",
    "print(\"GD\", ind(res.F))\n",
    "ind2 = IGD(pf)\n",
    "print(\"IGD\", ind2(res.F))\n",
    "\n",
    "\n",
    "n_evals = np.array([e.evaluator.n_eval for e in res.history])\n",
    "opt = np.array([e.opt[0].F for e in res.history])\n",
    "print(opt)\n",
    "plt.title(\"Convergence\")\n",
    "plt.plot(n_evals, opt, \"--\")\n",
    "plt.plot(n_evals, np.repeat(pf[0],len(n_evals)), 'k-', lw=1,dashes=[2, 2])\n",
    "#plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(torch.tensor([102.5045]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(102.5045)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
